{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Transformer Architecture for Shakespearean Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Project Overview and Motivation](#project-overview-and-motivation)\n",
    "3. [Objectives and Goals](#objectives-and-goals)\n",
    "4. [Dependencies and Setup](#dependencies-and-setup)\n",
    "5. [Data Acquisition and Preprocessing](#data-acquisition-and-preprocessing)\n",
    "    - [Brief Description of the Dataset](#brief-description-of-the-dataset)\n",
    "    - [Downloading the Shakespeare Text](#downloading-the-shakespeare-text)\n",
    "    - [Text Preprocessing](#text-preprocessing)\n",
    "    - [Vocabulary Creation](#vocabulary-creation)\n",
    "    - [Text Encoding](#text-encoding)\n",
    "    - [Data Splitting](#data-splitting)\n",
    "    - [Sequence Creation for Language Modeling](#sequence-creation-for-language-modeling)\n",
    "    - [Saving Vocabulary Mappings](#saving-vocabulary-mappings)\n",
    "    - [Saving Preprocessed Data](#saving-preprocessed-data)\n",
    "6. [Data Augmentation Techniques](#data-augmentation-techniques)\n",
    "    - [1. Word Dropout](#1-word-dropout)\n",
    "    - [2. Random Token Swapping](#2-random-token-swapping)\n",
    "    - [3. Combined Augmentation](#3-combined-augmentation)\n",
    "7. [TensorFlow Dataset Creation](#tensorflow-dataset-creation)\n",
    "8. [Transformer Architecture](#transformer-architecture)\n",
    "    - [Key Components of the Transformer](#key-components-of-the-transformer)\n",
    "    - [Positional Encoding](#positional-encoding)\n",
    "    - [Scaled Dot-Product Attention](#scaled-dot-product-attention)\n",
    "    - [Multi-Head Attention Implementation](#multi-head-attention-implementation)\n",
    "    - [Feed-Forward Networks](#feed-forward-networks)\n",
    "9. [Encoder Components](#encoder-components)\n",
    "    - [Encoder Layer](#encoder-layer)\n",
    "    - [Complete Encoder Stack](#complete-encoder-stack)\n",
    "10. [Decoder Components](#decoder-components)\n",
    "    - [Decoder Layer](#decoder-layer)\n",
    "    - [Complete Decoder Stack](#complete-decoder-stack)\n",
    "11. [Masking Functions](#masking-functions)\n",
    "    - [Padding Mask Creation](#padding-mask-creation)\n",
    "    - [Look-Ahead Mask Creation](#look-ahead-mask-creation)\n",
    "    - [Combined Masking](#combined-masking)\n",
    "12. [Complete Transformer Model](#complete-transformer-model)\n",
    "    - [Configuring the Transformer for Word-Level Language Modeling](#configuring-the-transformer-for-word-level-language-modeling)\n",
    "13. [Training Implementation](#training-implementation)\n",
    "    - [Loss Function with Label Smoothing](#loss-function-with-label-smoothing)\n",
    "    - [Custom Learning Rate Scheduler](#custom-learning-rate-scheduler)\n",
    "    - [Optimizer Configuration](#optimizer-configuration)\n",
    "    - [Training Step Implementation](#training-step-implementation)\n",
    "    - [Evaluation Function](#evaluation-function)\n",
    "14. [Training Process](#training-process)\n",
    "    - [Training Loop Details](#training-loop-details)\n",
    "    - [Sample Text Generation During Training](#sample-text-generation-during-training)\n",
    "15. [Results and Analysis](#results-and-analysis)\n",
    "    - [Training Process and Metrics](#training-process-and-metrics)\n",
    "    - [Performance Analysis](#performance-analysis)\n",
    "    - [Text Generation Quality](#text-generation-quality)\n",
    "    - [Overfitting Analysis](#overfitting-analysis)\n",
    "    - [Conclusion](#conclusion)\n",
    "16. [Training Metrics Visualization](#training-metrics-visualization)\n",
    "17. [Model Persistence](#model-persistence)\n",
    "    - [Saving Model and Configuration](#saving-model-and-configuration)\n",
    "    - [Loading Model from Saved Files](#loading-model-from-saved-files)\n",
    "18. [Model Evaluation](#model-evaluation)\n",
    "    - [Perplexity Calculation](#perplexity-calculation)\n",
    "    - [Generated Text Quality Assessment](#generated-text-quality-assessment)\n",
    "    - [Performance Benchmarking](#performance-benchmarking)\n",
    "19. [Conclusion and Future Work](#conclusion-and-future-work)\n",
    "    - [Discussion of Observations and Limitations](#discussion-of-observations-and-limitations)\n",
    "    - [Future Improvements and Experimentation Ideas](#future-improvements-and-experimentation-ideas)\n",
    "20. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The Transformer architecture, introduced in the landmark paper \"Attention Is All You Need\" by Vaswani et al. (2017), revolutionized natural language processing by demonstrating that sequence modeling tasks could be effectively tackled without recurrent or convolutional neural networks. This project implements a complete Transformer model from scratch for Shakespearean text generation, showcasing the architecture's ability to capture long-range dependencies and generate coherent, stylistically consistent text.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview and Motivation\n",
    "\n",
    "This implementation serves as both an educational exploration of the Transformer architecture and a demonstration of its capabilities in creative text generation. By training on Shakespeare's works, we aim to create a model that can generate text with similar linguistic patterns, vocabulary, and stylistic elements as the original author. The Transformer is particularly well-suited for this task due to its:\n",
    "\n",
    "1. **Parallel processing capabilities**: Unlike RNNs, Transformers process entire sequences simultaneously, making training more efficient.\n",
    "2. **Attention mechanisms**: The self-attention mechanism allows the model to weigh the importance of different words in the input sequence regardless of their distance, capturing long-range dependencies more effectively than RNNs.\n",
    "3. **Scalability**: The architecture can be scaled to handle larger datasets and more complex patterns by adjusting the number of layers, attention heads, and model dimensions.\n",
    "\n",
    "This project implements the complete Transformer architecture with both encoder and decoder components, following the original paper's design while making appropriate adjustments for word-level language modeling of Shakespearean text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives and Goals\n",
    "\n",
    "The primary objectives of this project are:\n",
    "\n",
    "1. **Implement a complete Transformer model**: Build all components of the Transformer architecture from scratch, including multi-head attention, positional encoding, feed-forward networks, and the full encoder-decoder structure.\n",
    "\n",
    "2. **Train on Shakespearean text**: Process and prepare Shakespeare's works for training a language model that captures the unique patterns and style of Elizabethan English.\n",
    "\n",
    "3. **Generate coherent Shakespearean-style text**: Demonstrate the model's ability to generate text that maintains thematic consistency and mimics Shakespeare's writing style.\n",
    "\n",
    "4. **Explore hyperparameter effects**: Examine how different model configurations, training strategies, and generation parameters affect the quality and diversity of the generated text.\n",
    "\n",
    "5. **Analyze model performance**: Evaluate the model using appropriate metrics like perplexity and qualitative assessment of generated samples.\n",
    "\n",
    "Through this implementation, we aim to gain deeper insights into how attention-based models learn linguistic patterns and demonstrate the Transformer's effectiveness for creative text generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Setup\n",
    "\n",
    "This implementation requires several key libraries to build and train our Transformer model:\n",
    "\n",
    "- **TensorFlow**: The core deep learning framework used for model implementation, providing efficient tensor operations and automatic differentiation capabilities.\n",
    "- **NumPy**: Used for numerical computations and array manipulations outside of TensorFlow's eager execution.\n",
    "- **Matplotlib**: Visualization library for plotting training metrics and model performance.\n",
    "- **Requests**: HTTP library for downloading the Shakespeare dataset.\n",
    "- **JSON**: For saving and loading model configurations and vocabulary mappings.\n",
    "- **Time**: For tracking training duration and benchmarking inference speed.\n",
    "\n",
    "The code is designed to run on both CPU and GPU environments, though training will be significantly faster with GPU acceleration. The implementation uses TensorFlow's eager execution mode for intuitive debugging and development, while leveraging `@tf.function` decorators for performance-critical operations during training.\n",
    "\n",
    "All dependencies are standard Python libraries that can be installed via pip if not already available. The model architecture is implemented from scratch following the original paper specifications, without relying on high-level APIs like Keras Layers for the core Transformer components, providing full transparency into the implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:28.661913Z",
     "iopub.status.busy": "2025-02-27T09:43:28.661344Z",
     "iopub.status.idle": "2025-02-27T09:43:40.226937Z",
     "shell.execute_reply": "2025-02-27T09:43:40.226224Z",
     "shell.execute_reply.started": "2025-02-27T09:43:28.661882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf         # Deep learning framework\n",
    "import numpy as np              # Numerical computations\n",
    "import matplotlib.pyplot as plt # Visualization\n",
    "import requests                 # HTTP requests for data downloading\n",
    "import os                       # File and directory operations\n",
    "import time                     # Time tracking for benchmarking\n",
    "import json                     # JSON handling for model config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Preprocessing\n",
    "\n",
    "The foundation of our Transformer model's training is high-quality text data that captures the unique linguistic patterns of Shakespeare's writing. This section details how we acquire, clean, and prepare Shakespeare's works for training our language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Description of the Dataset\n",
    "\n",
    "For this project, we use the \"Tiny Shakespeare\" dataset, a condensed corpus of Shakespeare's works commonly used for language modeling tasks. This dataset contains approximately 1 million characters of text from various Shakespeare plays and sonnets, providing a rich source of Elizabethan English with its distinctive vocabulary, syntax, and stylistic elements.\n",
    "\n",
    "The dataset includes dialogue from various characters, stage directions, and scene descriptions, offering a diverse range of linguistic patterns for our model to learn. While relatively small compared to modern language modeling datasets (which often contain billions of tokens), Tiny Shakespeare provides sufficient data to demonstrate the capabilities of the Transformer architecture while remaining computationally tractable for training without specialized hardware.\n",
    "\n",
    "The text exhibits several characteristics that make it an interesting challenge for language modeling:\n",
    "\n",
    "1. Archaic vocabulary and grammatical constructions\n",
    "2. Poetic meter and rhyme schemes in certain sections\n",
    "3. Distinctive character speech patterns\n",
    "4. Formal and informal language variations\n",
    "5. Rich metaphorical and figurative language\n",
    "\n",
    "These features provide an excellent test case for evaluating how well our Transformer model can capture and reproduce complex linguistic patterns across different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Shakespeare Text\n",
    "\n",
    "To train our Transformer model, we first need to acquire the Shakespeare dataset. We'll use the \"Tiny Shakespeare\" dataset, which is a condensed corpus of Shakespeare's works commonly used for language modeling experiments. This dataset is hosted in Andrej Karpathy's char-rnn repository and contains approximately 1 million characters of text.\n",
    "\n",
    "The function below handles the downloading process, creating a data directory if needed, and saving the text to a local file. It also performs basic validation by displaying the text length and a sample of the content to ensure the download was successful. This approach allows us to cache the dataset locally, avoiding redundant downloads in future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:40.228333Z",
     "iopub.status.busy": "2025-02-27T09:43:40.227915Z",
     "iopub.status.idle": "2025-02-27T09:43:40.678006Z",
     "shell.execute_reply": "2025-02-27T09:43:40.677119Z",
     "shell.execute_reply.started": "2025-02-27T09:43:40.228303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download and load the Shakespeare text data\n",
    "def download_shakespeare_data():\n",
    "    \"\"\"\n",
    "    Download and process Shakespeare text data from Karpathy's char-rnn repository.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete Shakespeare text data as a string.\n",
    "    \"\"\"\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "    # Create data directory if it doesn't exist\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "\n",
    "    file_path = os.path.join('data', 'shakespeare.txt')\n",
    "\n",
    "    # Download data if not already present\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Downloading Shakespeare text data...\")\n",
    "        response = requests.get(url)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"Downloaded and saved to {file_path}\")\n",
    "    else:\n",
    "        print(f\"Shakespeare data already exists at {file_path}\")\n",
    "\n",
    "    # Read the downloaded text data\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        shakespeare_text = file.read()\n",
    "\n",
    "    # Display basic information about the data\n",
    "    print(f\"Text length: {len(shakespeare_text)} characters\")\n",
    "    print(f\"First 100 characters: \\n{shakespeare_text[:100]}\")\n",
    "\n",
    "    return shakespeare_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Shakespeare data\n",
    "shakespeare_text = download_shakespeare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "After downloading the Shakespeare text, we need to preprocess it to make it suitable for training our Transformer model. The preprocessing steps include:\n",
    "\n",
    "1. **Cleaning the text**: Replacing line breaks with spaces, normalizing whitespace, and converting to lowercase for consistency.\n",
    "\n",
    "2. **Handling punctuation**: Adding spaces around punctuation marks to treat them as separate tokens, which helps the model learn the grammatical structure more effectively.\n",
    "\n",
    "3. **Tokenization**: Splitting the text into individual words (tokens) that will serve as the basic units for our language model.\n",
    "\n",
    "This word-level tokenization approach offers a good balance between model complexity and linguistic understanding. Unlike character-level models that must learn spelling patterns from scratch, word-level models can focus on learning grammatical structures and semantic relationships between words. This is particularly important for Shakespeare's text, which contains rich vocabulary and complex sentence structures.\n",
    "\n",
    "The function below implements these preprocessing steps and returns a list of words from the Shakespeare text. We'll use this list to build our vocabulary and create training examples for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:40.680035Z",
     "iopub.status.busy": "2025-02-27T09:43:40.679740Z",
     "iopub.status.idle": "2025-02-27T09:43:40.735229Z",
     "shell.execute_reply": "2025-02-27T09:43:40.734521Z",
     "shell.execute_reply.started": "2025-02-27T09:43:40.680012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Perform basic text preprocessing to prepare data for tokenization.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw text data to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed words from the text.\n",
    "    \"\"\"\n",
    "    # Replace line breaks with spaces for better tokenization\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Replace multiple consecutive spaces with a single space\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Convert text to lowercase for consistency\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add spaces around punctuation for better tokenization\n",
    "    punctuation_marks = ['.', ',', '!', '?', ':', ';', '\"', '(', ')', '[', ']', '{', '}']\n",
    "    for punctuation in punctuation_marks:\n",
    "        text = text.replace(punctuation, f\" {punctuation} \")\n",
    "\n",
    "    # Split text into individual words\n",
    "    words = text.split()\n",
    "\n",
    "    # Display information about processed words\n",
    "    print(f\"Total words after preprocessing: {len(words)}\")\n",
    "    print(f\"Sample words: {words[:13]}\")\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Shakespeare text to get a list of words\n",
    "processed_words = preprocess_text(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Creation\n",
    "\n",
    "After preprocessing the text into individual words, we need to create a vocabulary that maps each unique word to a numerical index. This vocabulary serves as the foundation for our language model, defining the set of tokens the model can recognize and generate.\n",
    "\n",
    "The vocabulary creation process involves:\n",
    "\n",
    "1. **Counting word frequencies**: We count how often each word appears in the text to identify the most common words.\n",
    "\n",
    "2. **Vocabulary size limitation**: To manage computational complexity, we limit our vocabulary to the most frequent words (up to a maximum size), which typically cover the vast majority of the text.\n",
    "\n",
    "3. **Special token addition**: We add special tokens to handle specific cases:\n",
    "   - `<PAD>`: Used for padding sequences to a consistent length in batches\n",
    "   - `<UNK>`: Used for unknown/rare words not included in the vocabulary\n",
    "\n",
    "4. **Word-to-index and index-to-word mappings**: We create bidirectional mappings between words and their corresponding indices, which are essential for encoding input text and decoding model outputs.\n",
    "\n",
    "This approach balances vocabulary coverage with model efficiency. While a larger vocabulary can represent more unique words directly, it also increases the model's parameter count and computational requirements. Our implementation focuses on capturing the most important words in Shakespeare's vocabulary while using the `<UNK>` token to handle rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:40.736644Z",
     "iopub.status.busy": "2025-02-27T09:43:40.736427Z",
     "iopub.status.idle": "2025-02-27T09:43:40.800474Z",
     "shell.execute_reply": "2025-02-27T09:43:40.799792Z",
     "shell.execute_reply.started": "2025-02-27T09:43:40.736626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create vocabulary from processed text\n",
    "def create_word_vocabulary(words, max_vocab_size=20000):\n",
    "    \"\"\"\n",
    "    Create vocabulary mappings from processed words.\n",
    "\n",
    "    Args:\n",
    "        words (list): List of preprocessed words.\n",
    "        max_vocab_size (int): Maximum size of the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (word-to-index mapping, index-to-word mapping, vocabulary size)\n",
    "    \"\"\"\n",
    "    # Count frequency of each word in the text\n",
    "    word_frequencies = {}\n",
    "    for word in words:\n",
    "        word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "    \n",
    "    # Sort words by frequency (most frequent first)\n",
    "    sorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Build vocabulary with special tokens and most frequent words\n",
    "    # <PAD>: Used for padding sequences to Used for unknown/rare words not in vocabulary\n",
    "    vocabulary = [\"<PAD>\", \"<UNK>\"] + [word for word, count in sorted_words[:max_vocab_size-2]]\n",
    "    \n",
    "    # Create word-to-index and index-to-word mappings\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    index_to_word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "    \n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    # Display information about the created vocabulary\n",
    "    print(f\"Vocabulary size: {vocabulary_size} words\")\n",
    "    print(f\"Sample word-to-index mappings: {list(word_to_index.items())[:5]}\")\n",
    "\n",
    "    return word_to_index, index_to_word, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word-to-index and index-to-word mappings, and get vocabulary size\n",
    "word2idx, idx2word, vocab_size = create_word_vocabulary(processed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoding\n",
    "\n",
    "After creating our vocabulary, we need to convert the preprocessed text into numerical sequences that our Transformer model can process. This encoding step transforms each word in our text into its corresponding index from the vocabulary.\n",
    "\n",
    "The encoding process serves several important purposes:\n",
    "\n",
    "1. **Numerical representation**: Neural networks operate on numerical data, so we need to convert text into numbers.\n",
    "\n",
    "2. **Vocabulary mapping**: Words not present in our vocabulary (rare words or those excluded due to vocabulary size limitations) are mapped to the special `<UNK>` token, ensuring our model can handle any input text.\n",
    "\n",
    "3. **Consistent representation**: By using the same word-to-index mapping throughout training and inference, we maintain consistency in how the model interprets text.\n",
    "\n",
    "This encoding step is the final transformation before we can create training examples for our model. The function below handles this conversion, taking our preprocessed words and mapping each one to its corresponding index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:40.801399Z",
     "iopub.status.busy": "2025-02-27T09:43:40.801167Z",
     "iopub.status.idle": "2025-02-27T09:43:40.834811Z",
     "shell.execute_reply": "2025-02-27T09:43:40.834131Z",
     "shell.execute_reply.started": "2025-02-27T09:43:40.801379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encode text using vocabulary\n",
    "def encode_text(words, word2idx):\n",
    "    \"\"\"\n",
    "    Encode text using word-to-index mapping from vocabulary.\n",
    "\n",
    "    Args:\n",
    "        words (list): List of preprocessed words.\n",
    "        word2idx (dict): Word-to-index mapping.\n",
    "\n",
    "    Returns:\n",
    "        list: List of encoded words (indices).\n",
    "    \"\"\"\n",
    "    # Convert each word to its index, using <UNK> for words not in vocabulary\n",
    "    return [word2idx.get(word, word2idx[\"<UNK>\"]) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text\n",
    "encoded_text = encode_text(processed_words, word2idx)\n",
    "print(f\"Encoded text length: {len(encoded_text)}\")\n",
    "print(f\"Sample encoded text: {encoded_text[:13]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "After encoding our text, we need to divide it into separate datasets for training, validation, and testing. This division is crucial for properly evaluating our model's performance:\n",
    "\n",
    "1. **Training set**: The largest portion of the data, used to train the model's parameters. The model learns patterns and relationships directly from this data.\n",
    "\n",
    "2. **Validation set**: Used during training to tune hyperparameters and monitor for overfitting. This helps us make decisions about model architecture and training process without contaminating our final evaluation.\n",
    "\n",
    "3. **Testing set**: Held-out data that the model never sees during training or validation. This provides an unbiased evaluation of the final model's performance on new, unseen text.\n",
    "\n",
    "We'll use a standard split ratio of 80% for training, 10% for validation, and 10% for testing. This balance provides sufficient data for training while reserving enough for meaningful validation and testing.\n",
    "\n",
    "The function below implements this splitting process, ensuring that the chronological order of the text is preserved. This is important for language modeling, as it maintains the narrative flow and contextual relationships within each subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:40.835906Z",
     "iopub.status.busy": "2025-02-27T09:43:40.835574Z",
     "iopub.status.idle": "2025-02-27T09:43:40.853667Z",
     "shell.execute_reply": "2025-02-27T09:43:40.852931Z",
     "shell.execute_reply.started": "2025-02-27T09:43:40.835876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets\n",
    "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split encoded data into training, validation, and testing sets.\n",
    "\n",
    "    Args:\n",
    "        data (list): Encoded text data.\n",
    "        train_ratio (float): Proportion of data to use for training.\n",
    "        val_ratio (float): Proportion of data to use for validation.\n",
    "        test_ratio (float): Proportion of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (training data, validation data, testing data)\n",
    "    \"\"\"\n",
    "    # Ensure ratios sum to 1.0\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Data split ratios must sum to 1.0\"\n",
    "\n",
    "    # Convert data to numpy array for easier manipulation\n",
    "    data_array = np.array(data, dtype=np.int32)\n",
    "\n",
    "    # Calculate split indices\n",
    "    data_size = len(data_array)\n",
    "    train_size = int(data_size * train_ratio)\n",
    "    val_size = int(data_size * val_ratio)\n",
    "\n",
    "    # Split data into training, validation, and testing sets\n",
    "    train_data = data_array[:train_size]\n",
    "    val_data = data_array[train_size:train_size+val_size]\n",
    "    test_data = data_array[train_size+val_size:]\n",
    "\n",
    "    # Display information about the data splits\n",
    "    print(f\"Training set size: {len(train_data)} tokens\")\n",
    "    print(f\"Validation set size: {len(val_data)} tokens\")\n",
    "    print(f\"Testing set size: {len(test_data)} tokens\")\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_data, val_data, test_data = split_data(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Creation for Language Modeling\n",
    "\n",
    "After splitting our encoded text into training, validation, and testing sets, we need to create input-target sequence pairs for language modeling. This step is crucial as it transforms our linear text data into a format suitable for training a sequence-to-sequence model.\n",
    "\n",
    "For language modeling with the Transformer architecture, we need to create pairs of sequences where:\n",
    "\n",
    "1. **Input sequence**: A fixed-length sequence of tokens that serves as context\n",
    "2. **Target sequence**: The same sequence shifted by one position, representing what the model should predict\n",
    "\n",
    "This sliding window approach allows the model to learn to predict the next word given a context of previous words. The process involves:\n",
    "\n",
    "1. **Sequence extraction**: Creating overlapping sequences of fixed length from our encoded text\n",
    "2. **Input-target pairing**: For each input sequence [x₀, x₁, ..., xₙ₋₁], creating a target sequence [x₁, x₂, ..., xₙ]\n",
    "3. **Efficient batching**: Organizing these pairs into batches for efficient training\n",
    "\n",
    "The function below implements this sequence creation process, with a configurable step size that determines the overlap between consecutive sequences. A smaller step size creates more training examples with greater overlap, potentially helping the model learn more effectively from limited data, while a larger step size reduces redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:40.854776Z",
     "iopub.status.busy": "2025-02-27T09:43:40.854484Z",
     "iopub.status.idle": "2025-02-27T09:43:41.278430Z",
     "shell.execute_reply": "2025-02-27T09:43:41.277676Z",
     "shell.execute_reply.started": "2025-02-27T09:43:40.854749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create input-target pairs for sequence modeling\n",
    "def create_sequences(data, seq_length, step=1):\n",
    "    \"\"\"\n",
    "    Create input-target sequence pairs for language modeling.\n",
    "\n",
    "    For each sequence of length seq_length in the data, creates:\n",
    "    - Input sequence: [x_0, x_1, ..., x_{seq_length-1}]\n",
    "    - Target sequence: [x_1, x_2, ..., x_{seq_length}]\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): Encoded token sequence.\n",
    "        seq_length (int): Fixed length for each input sequence.\n",
    "        step (int): Stride between starting positions of consecutive sequences.\n",
    "                    Default is 1 for maximum overlap between sequences.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (input sequences array, target sequences array)\n",
    "    \"\"\"\n",
    "    input_sequences = []\n",
    "    target_sequences = []\n",
    "\n",
    "    # Create overlapping sequences from data\n",
    "    for start_idx in range(0, len(data) - seq_length, step):\n",
    "        # Input sequence: current seq_length tokens\n",
    "        input_seq = data[start_idx:start_idx + seq_length]\n",
    "        # Target sequence: next seq_length tokens (shifted by 1)\n",
    "        target_seq = data[start_idx + 1:start_idx + seq_length + 1]\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    input_sequences_array = np.array(input_sequences)\n",
    "    target_sequences_array = np.array(target_sequences)\n",
    "\n",
    "    # Display information about created sequences\n",
    "    print(f\"Number of sequence pairs: {len(input_sequences_array)}\")\n",
    "    print(f\"Input sequence shape: {input_sequences_array.shape}\")\n",
    "    print(f\"Target sequence shape: {target_sequences_array.shape}\")\n",
    "\n",
    "    return input_sequences_array, target_sequences_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for training, validation, and testing\n",
    "seq_length = 50\n",
    "\n",
    "train_inputs, train_targets = create_sequences(train_data, seq_length)\n",
    "val_inputs, val_targets = create_sequences(val_data, seq_length)\n",
    "test_inputs, test_targets = create_sequences(test_data, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Vocabulary Mappings\n",
    "\n",
    "Preserving the vocabulary mappings is a critical step in our language modeling pipeline. These mappings establish the connection between words in our text and their numerical representations in the model. By saving these mappings to disk, we ensure:\n",
    "\n",
    "1. **Consistency between runs**: The same vocabulary can be used across multiple training sessions, ensuring consistent model behavior.\n",
    "\n",
    "2. **Inference capability**: When using the trained model for text generation or other tasks, we need the exact same vocabulary mappings to properly encode inputs and decode outputs.\n",
    "\n",
    "3. **Model portability**: Saving these mappings alongside the model weights allows the model to be deployed in different environments while maintaining the same word-to-index relationships.\n",
    "\n",
    "4. **Reproducibility**: Preserving the vocabulary is essential for reproducing experimental results and comparing different model versions.\n",
    "\n",
    "The functions below handle saving our vocabulary mappings (word-to-index and index-to-word dictionaries) to JSON files, along with the vocabulary size. This information will be loaded during model inference to ensure that text processing remains consistent with what the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:41.281268Z",
     "iopub.status.busy": "2025-02-27T09:43:41.281045Z",
     "iopub.status.idle": "2025-02-27T09:43:41.323595Z",
     "shell.execute_reply": "2025-02-27T09:43:41.322916Z",
     "shell.execute_reply.started": "2025-02-27T09:43:41.281249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save vocabulary mappings to files\n",
    "def save_vocab(word2idx, idx2word, vocab_size, save_dir='data'):\n",
    "    \"\"\"\n",
    "    Save vocabulary mappings and metadata to JSON files.\n",
    "\n",
    "    Args:\n",
    "        word2idx (dict): Word-to-index mapping.\n",
    "        idx2word (dict): Index-to-word mapping.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        save_dir (str): Directory to save vocabulary files.\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Save word-to-index mapping\n",
    "    with open(os.path.join(save_dir, 'word2idx.json'), 'w') as file:\n",
    "        # Convert integer keys to strings for JSON serialization\n",
    "        word2idx_serializable = {word: int(idx) for word, idx in word2idx.items()}\n",
    "        json.dump(word2idx_serializable, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save index-to-word mapping\n",
    "    with open(os.path.join(save_dir, 'idx2word.json'), 'w') as file:\n",
    "        # Convert integer keys to strings for JSON serialization\n",
    "        idx2word_serializable = {str(idx): word for idx, word in idx2word.items()}\n",
    "        json.dump(idx2word_serializable, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save vocabulary size and metadata\n",
    "    with open(os.path.join(save_dir, 'vocab_info.json'), 'w') as file:\n",
    "        json.dump({\n",
    "            'vocab_size': vocab_size,\n",
    "            'special_tokens': {\n",
    "                'pad_token': '<PAD>',\n",
    "                'unknown_token': '<UNK>',\n",
    "                'pad_token_id': word2idx['<PAD>'],\n",
    "                'unknown_token_id': word2idx['<UNK>']\n",
    "            }\n",
    "        }, file)\n",
    "\n",
    "    print(f\"Vocabulary files saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary mappings and size\n",
    "save_vocab(word2idx, idx2word, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Preprocessed Data\n",
    "\n",
    "After creating our input-target sequence pairs and splitting them into training, validation, and testing sets, it's important to save these preprocessed data arrays to disk. This step provides several key benefits:\n",
    "\n",
    "1. **Computational efficiency**: Preprocessing text data can be time-consuming, especially for large corpora. By saving the results, we avoid repeating these operations in future sessions.\n",
    "\n",
    "2. **Reproducibility**: Storing preprocessed data ensures that different training runs use exactly the same data, making experimental results more comparable and reproducible.\n",
    "\n",
    "3. **Workflow flexibility**: Saving intermediate data allows us to experiment with different model architectures or training configurations without redoing the preprocessing steps.\n",
    "\n",
    "4. **Checkpoint capability**: If our workflow is interrupted, we can resume from the saved data rather than starting from scratch.\n",
    "\n",
    "The functions below handle saving our preprocessed arrays (training, validation, and testing inputs and targets) to NumPy files. These files can be quickly loaded in subsequent sessions, streamlining the development and experimentation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:41.325387Z",
     "iopub.status.busy": "2025-02-27T09:43:41.325080Z",
     "iopub.status.idle": "2025-02-27T09:43:41.396291Z",
     "shell.execute_reply": "2025-02-27T09:43:41.395564Z",
     "shell.execute_reply.started": "2025-02-27T09:43:41.325366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save prepared data arrays\n",
    "def save_numpy_arrays(arrays, filenames, save_dir='data'):\n",
    "    \"\"\"\n",
    "    Save numpy arrays to files for future use.\n",
    "\n",
    "    Args:\n",
    "        arrays (list): List of numpy arrays to save.\n",
    "        filenames (list): List of filenames for each array.\n",
    "        save_dir (str): Directory to save arrays.\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Save each array to a file\n",
    "    for array, filename in zip(arrays, filenames):\n",
    "        file_path = os.path.join(save_dir, filename)\n",
    "        np.save(file_path, array)\n",
    "\n",
    "    print(f\"Data arrays saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the preprocessed data arrays \n",
    "save_numpy_arrays(\n",
    "        [train_inputs, train_targets, val_inputs, val_targets, test_inputs, test_targets],\n",
    "        ['train_inputs.npy', 'train_targets.npy', 'val_inputs.npy', 'val_targets.npy', 'test_inputs.npy', 'test_targets.npy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation Techniques\n",
    "\n",
    "Data augmentation is a powerful strategy to improve model generalization by artificially expanding the training dataset with modified versions of existing examples. For natural language processing tasks like our Shakespeare language modeling, appropriate augmentation techniques can help the model become more robust to variations in text and reduce overfitting, especially when working with limited data.\n",
    "\n",
    "For our Transformer model, we'll implement three complementary augmentation techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word Dropout\n",
    "\n",
    "Word dropout randomly replaces a percentage of words in the input sequences with the `<UNK>` (unknown) token. This technique serves multiple purposes:\n",
    "\n",
    "- **Improves robustness**: Forces the model to learn to predict the next word even when parts of the context are missing\n",
    "- **Simulates rare words**: Helps the model handle situations where it encounters words outside its vocabulary\n",
    "- **Reduces overfitting**: Prevents the model from relying too heavily on specific words in fixed positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word dropout function\n",
    "def apply_word_dropout(sequences, word2idx, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Randomly replace words with <UNK> token to improve model robustness.\n",
    "    \n",
    "    Args:\n",
    "        sequences (np.ndarray): Input sequences of token IDs\n",
    "        word2idx (dict): Word-to-index mapping\n",
    "        dropout_rate (float): Probability of replacing a word with <UNK>\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Augmented sequences with some words replaced by <UNK>\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    augmented_sequences = sequences.copy()\n",
    "    \n",
    "    # Get token IDs for special tokens\n",
    "    unk_id = word2idx[\"<UNK>\"]\n",
    "    pad_id = word2idx[\"<PAD>\"]\n",
    "    \n",
    "    # Create random mask for dropout (True where we should drop)\n",
    "    mask = np.random.random(sequences.shape) < dropout_rate\n",
    "    \n",
    "    # Only drop non-padding tokens\n",
    "    effective_mask = (sequences != pad_id) & mask\n",
    "    \n",
    "    # Replace dropped tokens with <UNK>\n",
    "    augmented_sequences[effective_mask] = unk_id\n",
    "    \n",
    "    return augmented_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Token Swapping\n",
    "\n",
    "This technique randomly swaps adjacent tokens in the input sequences, creating slight variations in word order. Benefits include:\n",
    "\n",
    "- **Syntactic variation**: Exposes the model to different grammatical structures\n",
    "- **Reduces positional bias**: Prevents the model from overly relying on exact word positions\n",
    "- **Increases linguistic diversity**: Creates new sentence structures while preserving most of the semantic content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly swaps adjacent tokens function\n",
    "def apply_random_swaps(sequences, swap_rate=0.1):\n",
    "    \"\"\"\n",
    "    Randomly swap adjacent words to create more diverse training data.\n",
    "    \n",
    "    Args:\n",
    "        sequences (np.ndarray): Input sequences of token IDs\n",
    "        swap_rate (float): Probability of swapping adjacent words\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Augmented sequences with some adjacent words swapped\n",
    "    \"\"\"\n",
    "    augmented_sequences = sequences.copy()\n",
    "    seq_length = sequences.shape[1]\n",
    "    \n",
    "    # For each sequence in the batch\n",
    "    for i in range(len(sequences)):\n",
    "        # Consider each position except the last one\n",
    "        for j in range(seq_length - 1):\n",
    "            # Apply swap with probability swap_rate\n",
    "            if np.random.random() < swap_rate:\n",
    "                # Swap the current token with the next one\n",
    "                augmented_sequences[i, j], augmented_sequences[i, j+1] = \\\n",
    "                    augmented_sequences[i, j+1], augmented_sequences[i, j]\n",
    "    \n",
    "    return augmented_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combined Augmentation\n",
    "\n",
    "By applying both techniques with controlled probability, we create a diverse set of augmented examples that maintain the core meaning of the original text while introducing controlled variations. This balanced approach helps the model generalize better without distorting the linguistic patterns too severely.\n",
    "\n",
    "The functions below implement these augmentation techniques, with configurable parameters to control the intensity of each transformation. We'll apply these augmentations only to the training data, keeping the validation and test sets in their original form to properly evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation function\n",
    "def augment_data(inputs, targets, word2idx, dropout_rate=0.1, swap_rate=0.1, verbose=True):\n",
    "    \"\"\"\n",
    "    Create augmented training data using word dropout and random swaps.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input sequences\n",
    "        targets: Target sequences\n",
    "        word2idx: Word to index mapping dictionary\n",
    "        dropout_rate: Probability of dropping a word\n",
    "        swap_rate: Probability of swapping adjacent words\n",
    "        verbose: Whether to print information about the augmentation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (augmented_inputs, augmented_targets)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Applying data augmentation techniques...\")\n",
    "\n",
    "    # Apply word dropout\n",
    "    augmented_inputs_1 = apply_word_dropout(inputs, word2idx, dropout_rate=dropout_rate)\n",
    "\n",
    "    # Apply random swaps\n",
    "    augmented_inputs_2 = apply_random_swaps(inputs, swap_rate=swap_rate)\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    augmented_inputs = np.concatenate([\n",
    "        inputs,             # Original data\n",
    "        augmented_inputs_1, # Word dropout augmentation\n",
    "        augmented_inputs_2  # Word swap augmentation\n",
    "    ], axis=0)\n",
    "\n",
    "    # Repeat targets to match the augmented inputs\n",
    "    augmented_targets = np.concatenate([\n",
    "        targets,\n",
    "        targets,\n",
    "        targets\n",
    "    ], axis=0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Original dataset size: {len(inputs)} sequences\")\n",
    "        print(f\"Augmented dataset size: {len(augmented_inputs)} sequences\")\n",
    "        \n",
    "    return augmented_inputs, augmented_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation to training data\n",
    "augmented_train_inputs, augmented_train_targets = augment_data(train_inputs, train_targets, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Dataset Creation\n",
    "\n",
    "After preparing and augmenting our data, we need to convert it into efficient TensorFlow Dataset objects for training and evaluation. The TensorFlow Dataset API provides optimized data pipelines that can significantly improve training performance through features like prefetching, batching, and caching.\n",
    "\n",
    "For our Transformer model, we'll create three separate datasets:\n",
    "\n",
    "### 1. Training Dataset\n",
    "\n",
    "The training dataset requires special handling to optimize the learning process:\n",
    "- **Shuffling**: Randomizes the order of examples to prevent the model from learning sequence-specific patterns and improves training stability\n",
    "- **Batching**: Groups examples together for efficient parallel processing on GPU/TPU\n",
    "- **Prefetching**: Loads the next batch of data while the current batch is being processed, reducing idle time\n",
    "- **Caching**: Stores processed data in memory to avoid redundant computations\n",
    "\n",
    "### 2. Validation Dataset\n",
    "\n",
    "The validation dataset is used to evaluate the model during training:\n",
    "- No shuffling is applied to ensure consistent evaluation across epochs\n",
    "- Batching is still used for computational efficiency\n",
    "- The dataset size is typically smaller than the training set\n",
    "\n",
    "### 3. Test Dataset\n",
    "\n",
    "The test dataset is used only for final model evaluation:\n",
    "- Like the validation set, no shuffling is applied\n",
    "- Provides an unbiased assessment of model performance on unseen data\n",
    "\n",
    "The functions below implement this dataset creation process, converting our sequence pairs into TensorFlow Dataset objects with the appropriate configurations for each use case. This approach ensures efficient data handling during the training process, particularly important for larger datasets or when training on GPU/TPU accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:41.397266Z",
     "iopub.status.busy": "2025-02-27T09:43:41.397057Z",
     "iopub.status.idle": "2025-02-27T09:43:42.250533Z",
     "shell.execute_reply": "2025-02-27T09:43:42.249763Z",
     "shell.execute_reply.started": "2025-02-27T09:43:41.397249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets for efficient training\n",
    "def create_tf_dataset(inputs, targets, batch_size, buffer_size=10000, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create a TensorFlow dataset from input and target sequences.\n",
    "\n",
    "    Args:\n",
    "        inputs (numpy.ndarray): Input sequence data.\n",
    "        targets (numpy.ndarray): Target sequence data.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        buffer_size (int): Buffer size for shuffling.\n",
    "        shuffle (bool): Whether to shuffle the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: TensorFlow dataset for model training/evaluation.\n",
    "    \"\"\"\n",
    "    # Create dataset from tensors\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "\n",
    "    # Shuffle dataset if specified\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "\n",
    "    # Batch the dataset and drop remainder to ensure consistent batch sizes\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # Prefetch data for better performance\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets\n",
    "batch_size = 128\n",
    "train_dataset = create_tf_dataset(augmented_train_inputs, augmented_train_targets, batch_size)\n",
    "val_dataset = create_tf_dataset(val_inputs, val_targets, batch_size, shuffle=False)\n",
    "test_dataset = create_tf_dataset(test_inputs, test_targets, batch_size, shuffle=False)\n",
    "\n",
    "print(\"Datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "The Transformer architecture, introduced in the seminal paper \"Attention Is All You Need\" by Vaswani et al. (2017), represents a paradigm shift in sequence modeling. Unlike previous approaches that relied on recurrent or convolutional neural networks, Transformers use self-attention mechanisms to process input sequences in parallel, capturing long-range dependencies more effectively while enabling significantly faster training.\n",
    "\n",
    "### Key Components of the Transformer\n",
    "\n",
    "Our implementation follows the original architecture with some adaptations for language modeling:\n",
    "\n",
    "1. **Multi-Head Self-Attention**: The core innovation of the Transformer, allowing the model to attend to different positions in the input sequence simultaneously. Each attention head can focus on different aspects of the relationships between words.\n",
    "\n",
    "2. **Positional Encoding**: Since the Transformer processes all tokens in parallel (rather than sequentially like RNNs), positional encodings are added to provide information about the relative or absolute position of tokens in the sequence.\n",
    "\n",
    "3. **Feed-Forward Networks**: Each layer contains a position-wise feed-forward network that applies the same transformation to each position independently, adding non-linearity and representational power.\n",
    "\n",
    "4. **Residual Connections and Layer Normalization**: These components help stabilize training and allow for deeper networks by mitigating the vanishing gradient problem.\n",
    "\n",
    "5. **Encoder-Decoder Structure**: The complete Transformer consists of an encoder that processes the input sequence and a decoder that generates the output sequence, with cross-attention mechanisms connecting them.\n",
    "\n",
    "### Adaptations for Language Modeling\n",
    "\n",
    "For our Shakespeare text generation task, we make several adaptations to the original architecture:\n",
    "\n",
    "1. **Causal Attention Masking**: In the decoder, we apply masking to ensure that predictions for a given position can only depend on known outputs at earlier positions, which is essential for autoregressive generation.\n",
    "\n",
    "2. **Vocabulary Embeddings**: We use learned embeddings to convert token indices into continuous vector representations, with the same embedding layer used for both the encoder and decoder inputs.\n",
    "\n",
    "3. **Linear Output Projection**: The final decoder output is projected to logits over our vocabulary, representing the probability distribution for the next word.\n",
    "\n",
    "The following sections implement each component of the Transformer architecture, building from the fundamental attention mechanism to the complete encoder-decoder model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Unlike recurrent neural networks, the Transformer processes all tokens in a sequence simultaneously, which means it has no inherent understanding of token order. To address this limitation, positional encodings are added to the input embeddings to provide the model with information about the relative or absolute position of each token in the sequence.\n",
    "\n",
    "The standard approach, as described in the original \"Attention Is All You Need\" paper, uses sine and cosine functions of different frequencies to create unique positional encodings:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "\n",
    "Where:\n",
    "- $pos$ is the position of the token in the sequence\n",
    "- $i$ is the dimension index\n",
    "- $d_{model}$ is the embedding dimension\n",
    "\n",
    "This formulation has several important properties:\n",
    "\n",
    "1. **Uniqueness**: Each position gets a unique encoding, allowing the model to distinguish between different positions\n",
    "2. **Deterministic**: The encoding is fixed and doesn't require learning\n",
    "3. **Bounded values**: The values fall within [-1, 1], making them compatible with the scale of the embeddings\n",
    "4. **Theoretical extrapolation**: The sinusoidal pattern theoretically allows the model to extrapolate to sequence lengths longer than those seen during training\n",
    "\n",
    "The implementation below creates these positional encodings as a fixed tensor that can be added to the input embeddings. By incorporating this information at the input level, every layer in the Transformer can access positional information, enabling the model to learn position-dependent patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.251643Z",
     "iopub.status.busy": "2025-02-27T09:43:42.251326Z",
     "iopub.status.idle": "2025-02-27T09:43:42.256895Z",
     "shell.execute_reply": "2025-02-27T09:43:42.256123Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.251613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Positional encoding functions for Transformer\n",
    "def get_angles(positions, indices, d_model):\n",
    "    \"\"\"\n",
    "    Calculate angle rates for positional encoding.\n",
    "\n",
    "    Args:\n",
    "        positions (numpy.ndarray): Position indices [0, 1, 2, ...]\n",
    "        indices (numpy.ndarray): Dimension indices [0, 1, 2, ...]\n",
    "        d_model (int): Model dimension size.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Angle rates for positional encoding.\n",
    "    \"\"\"\n",
    "    # Calculate angle rates based on position and dimension\n",
    "    angle_rates = 1 / np.power(10000, (2 * (indices // 2)) / np.float32(d_model))\n",
    "    return positions * angle_rates\n",
    "\n",
    "def positional_encoding(max_position, d_model):\n",
    "    \"\"\"\n",
    "    Generate positional encoding for Transformer model.\n",
    "\n",
    "    The positional encoding is added to the embedding to provide\n",
    "    positional information since Transformer has no recurrence/convolution.\n",
    "\n",
    "    Args:\n",
    "        max_position (int): Maximum sequence length to generate encodings for.\n",
    "        d_model (int): Dimensionality of the model embeddings.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Positional encoding tensor of shape [1, max_position, d_model].\n",
    "    \"\"\"\n",
    "    # Calculate angle rates for all positions and dimensions\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(max_position)[:, np.newaxis],    # Position vector: [0, 1, 2, ...]\n",
    "        np.arange(d_model)[np.newaxis, :],         # Dimension vector: [0, 1, 2, ...]\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    # Apply sine to even dimensions and cosine to odd dimensions\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Even dimensions: sine\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Odd dimensions: cosine\n",
    "\n",
    "    # Add batch dimension [1, max_position, d_model]\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    # Convert to TensorFlow tensor with float32 precision\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "At the heart of the Transformer architecture lies the attention mechanism, specifically the scaled dot-product attention. This mechanism allows the model to focus on different parts of the input sequence when producing each element of the output sequence, effectively capturing dependencies regardless of their distance in the sequence.\n",
    "\n",
    "The scaled dot-product attention is computed as follows:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ (query), $K$ (key), and $V$ (value) are matrices representing different projections of the input\n",
    "- $d_k$ is the dimension of the keys (used for scaling)\n",
    "- The softmax is applied row-wise\n",
    "\n",
    "The attention operation can be broken down into these steps:\n",
    "\n",
    "1. **Compatibility calculation**: Compute dot products between each query and all keys ($QK^T$)\n",
    "2. **Scaling**: Divide by $\\sqrt{d_k}$ to prevent extremely small gradients when $d_k$ is large\n",
    "3. **Masking (optional)**: Apply a mask to prevent attention to certain positions (e.g., future positions in causal attention)\n",
    "4. **Softmax**: Apply softmax to obtain attention weights that sum to 1\n",
    "5. **Value weighting**: Multiply the attention weights by the values ($V$)\n",
    "\n",
    "The result is a weighted sum of the values, where the weights are determined by the compatibility of the corresponding keys with each query.\n",
    "\n",
    "For language modeling, we often use causal masking in the decoder to ensure that predictions for a given position can only depend on known outputs at earlier positions, which is essential for autoregressive generation.\n",
    "\n",
    "The implementation below creates a flexible attention function that supports both standard attention for the encoder and masked attention for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.257840Z",
     "iopub.status.busy": "2025-02-27T09:43:42.257614Z",
     "iopub.status.idle": "2025-02-27T09:43:42.269034Z",
     "shell.execute_reply": "2025-02-27T09:43:42.268298Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.257821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Scaled dot-product attention mechanism\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate scaled dot-product attention as described in the Transformer paper.\n",
    "\n",
    "    The attention formula is: Attention(Q, K, V) = softmax(QK^T/√d_k)V\n",
    "\n",
    "    Args:\n",
    "        query: Query tensor of shape (..., seq_len_q, depth).\n",
    "        key: Key tensor of shape (..., seq_len_k, depth).\n",
    "        value: Value tensor of shape (..., seq_len_v, depth_v).\n",
    "        mask: Optional mask tensor of shape broadcastable to (..., seq_len_q, seq_len_k).\n",
    "              Used to mask out certain attention connections (1 = keep, 0 = mask).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (attention output, attention weights)\n",
    "    \"\"\"\n",
    "    # Calculate dot product of query and key: (Q)(K^T)\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # Scale dot product by square root of depth\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # Apply mask if provided (adding large negative values to masked positions)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    # Compute weighted sum of values using attention weights\n",
    "    attention_output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Implementation\n",
    "\n",
    "While the scaled dot-product attention mechanism is powerful, the original Transformer paper introduced multi-head attention to further enhance the model's capability. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, enabling it to capture various aspects of the relationships between tokens.\n",
    "\n",
    "The multi-head attention mechanism works as follows:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "Where each head is computed as:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "In this formulation:\n",
    "- $W_i^Q$, $W_i^K$, and $W_i^V$ are learned projection matrices for the $i$-th head\n",
    "- $W^O$ is the output projection matrix\n",
    "- $h$ is the number of attention heads\n",
    "\n",
    "The key advantages of multi-head attention include:\n",
    "\n",
    "1. **Parallel attention mechanisms**: Each head can focus on different aspects of the input, similar to having multiple feature detectors\n",
    "2. **Increased representation power**: The model can simultaneously attend to information from different representation subspaces\n",
    "3. **Enhanced learning capacity**: Multiple heads provide more pathways for gradient flow during training\n",
    "\n",
    "Our implementation follows these steps:\n",
    "1. Project the inputs into multiple heads using learned linear transformations\n",
    "2. Apply scaled dot-product attention to each head independently\n",
    "3. Concatenate the results from all heads\n",
    "4. Apply a final linear projection to produce the output\n",
    "\n",
    "This implementation supports both self-attention (where Q, K, and V are the same sequence) and cross-attention (where Q comes from one sequence while K and V come from another), which are used in different parts of the Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.269896Z",
     "iopub.status.busy": "2025-02-27T09:43:42.269672Z",
     "iopub.status.idle": "2025-02-27T09:43:42.335075Z",
     "shell.execute_reply": "2025-02-27T09:43:42.334469Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.269879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Multi-head attention layer implementation\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-head attention layer as described in 'Attention Is All You Need'.\n",
    "\n",
    "    This layer splits the embedding dimension into multiple heads to allow\n",
    "    the model to jointly attend to information from different representation\n",
    "    subspaces at different positions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize multi-head attention layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Embedding dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Ensure d_model is divisible by num_heads\n",
    "        assert d_model % self.num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Depth of each attention head\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        # Dense layers for linear projections\n",
    "        self.query_layer = tf.keras.layers.Dense(d_model)  # Query projection\n",
    "        self.key_layer = tf.keras.layers.Dense(d_model)    # Key projection\n",
    "        self.value_layer = tf.keras.layers.Dense(d_model)  # Value projection\n",
    "        self.output_layer = tf.keras.layers.Dense(d_model) # Output projection\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth).\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Tensor with shape (batch_size, seq_len, d_model).\n",
    "            batch_size (int): Batch size.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Reshaped tensor with shape (batch_size, num_heads, seq_len, depth).\n",
    "        \"\"\"\n",
    "        # Reshape x to (batch_size, seq_len, num_heads, depth)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "\n",
    "        # Transpose to (batch_size, num_heads, seq_len, depth)\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, value, key, query, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            value (tf.Tensor): Value tensor.\n",
    "            key (tf.Tensor): Key tensor.\n",
    "            query (tf.Tensor): Query tensor.\n",
    "            mask (tf.Tensor, optional): Attention mask tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (attention output, attention weights)\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Linear projections\n",
    "        query_projected = self.query_layer(query)  # (batch_size, seq_len_q, d_model)\n",
    "        key_projected = self.key_layer(key)        # (batch_size, seq_len_k, d_model)\n",
    "        value_projected = self.value_layer(value)  # (batch_size, seq_len_v, d_model)\n",
    "\n",
    "        # Split projections into multiple heads\n",
    "        query_multi_head = self.split_heads(query_projected, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        key_multi_head = self.split_heads(key_projected, batch_size)      # (batch_size, num_heads, seq_len_k, depth)\n",
    "        value_multi_head = self.split_heads(value_projected, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # Apply scaled dot-product attention to each head\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            query_multi_head, key_multi_head, value_multi_head, mask)\n",
    "        # scaled_attention shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "\n",
    "        # Transpose and reshape back to original dimensions\n",
    "        # Transpose to (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention_transposed = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # Reshape to (batch_size, seq_len_q, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention_transposed, (batch_size, -1, self.d_model))\n",
    "\n",
    "        # Apply final output projection\n",
    "        output = self.output_layer(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks\n",
    "\n",
    "In addition to the attention mechanisms, each layer of the Transformer contains a position-wise feed-forward network (FFN). This component applies the same feed-forward transformation to each position independently, adding non-linearity and increasing the model's representational capacity.\n",
    "\n",
    "The feed-forward network consists of two linear transformations with a ReLU activation in between:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Where:\n",
    "- $W_1$ and $b_1$ are the weights and bias of the first linear transformation\n",
    "- $W_2$ and $b_2$ are the weights and bias of the second linear transformation\n",
    "- $\\max(0, \\cdot)$ represents the ReLU activation function\n",
    "\n",
    "Key characteristics of the feed-forward networks in Transformers:\n",
    "\n",
    "1. **Position-wise application**: The same transformation is applied to each position independently, preserving the sequence length\n",
    "2. **Dimensionality expansion**: Typically, the inner dimension ($d_{ff}$) is larger than the model dimension ($d_{model}$), allowing for more complex transformations\n",
    "3. **Non-linearity**: The ReLU activation introduces non-linearity, enabling the model to learn more complex functions\n",
    "4. **Parameter efficiency**: Despite the increased inner dimension, sharing parameters across positions keeps the total parameter count manageable\n",
    "\n",
    "The feed-forward network can be viewed as a 1x1 convolution with two layers, processing each position in the sequence independently while transforming the feature representation. This component complements the attention mechanism: while attention captures relationships between different positions, the feed-forward network processes each position's features more deeply.\n",
    "\n",
    "The implementation below creates a feed-forward network class that follows the original Transformer architecture specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.335889Z",
     "iopub.status.busy": "2025-02-27T09:43:42.335679Z",
     "iopub.status.idle": "2025-02-27T09:43:42.339307Z",
     "shell.execute_reply": "2025-02-27T09:43:42.338679Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.335872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Point-wise feed-forward network\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    \"\"\"\n",
    "    Create a point-wise feed-forward network for Transformer layers.\n",
    "\n",
    "    As specified in the Transformer paper, each layer contains a fully-connected\n",
    "    feed-forward network consisting of two linear transformations with a ReLU\n",
    "    activation in between.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Model dimension (input and output dimension).\n",
    "        dff (int): Inner layer dimension (typically 4*d_model).\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Sequential: Feed-forward network.\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        # First dense layer with ReLU activation\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "\n",
    "        # Second dense layer to restore original dimension\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Components\n",
    "\n",
    "The encoder is a fundamental component of the Transformer architecture, responsible for processing the input sequence and creating contextualized representations that capture the relationships between tokens. The encoder consists of multiple identical layers stacked on top of each other, each containing two main sub-layers: a multi-head self-attention mechanism and a position-wise feed-forward network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "Each encoder layer transforms its input through the following sequence of operations:\n",
    "\n",
    "1. **Multi-Head Self-Attention**: Allows each position to attend to all positions in the previous layer, capturing contextual relationships regardless of distance\n",
    "   - Input: Sequence representations from the previous layer\n",
    "   - Process: Computes attention scores between all pairs of positions\n",
    "   - Output: Context-aware representations for each position\n",
    "\n",
    "2. **Residual Connection and Layer Normalization**: After the attention sub-layer\n",
    "   - Residual connection: Adds the input to the output of the attention sub-layer, helping with gradient flow\n",
    "   - Layer normalization: Normalizes the features across the embedding dimension, stabilizing training\n",
    "\n",
    "3. **Position-wise Feed-Forward Network**: Applies the same feed-forward transformation to each position independently\n",
    "   - Input: Normalized output from the attention sub-layer\n",
    "   - Process: Two linear transformations with a ReLU activation in between\n",
    "   - Output: Transformed representations with enhanced feature extraction\n",
    "\n",
    "4. **Second Residual Connection and Layer Normalization**: After the feed-forward sub-layer\n",
    "   - Completes the layer by adding another residual connection and normalization\n",
    "\n",
    "This architecture allows each encoder layer to refine the representations from the previous layer, gradually building up a rich understanding of the input sequence.\n",
    "\n",
    "The implementation below creates both the individual encoder layer and the complete encoder stack, following the architecture described in the original Transformer paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.340223Z",
     "iopub.status.busy": "2025-02-27T09:43:42.340000Z",
     "iopub.status.idle": "2025-02-27T09:43:42.353527Z",
     "shell.execute_reply": "2025-02-27T09:43:42.352787Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.340204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encoder layer implementation\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder layer for the Transformer model.\n",
    "\n",
    "    Each encoder layer consists of:\n",
    "    1. Multi-head self-attention mechanism\n",
    "    2. Position-wise feed-forward network\n",
    "\n",
    "    Both sublayers have residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize encoder layer.\n",
    "\n",
    "        Args:\n",
    "        d_model (int): Model dimension.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        dff (int): Inner dimension of feed-forward network.\n",
    "        dropout_rate (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # Multi-head attention sublayer\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # Feed-forward network sublayer\n",
    "        self.feed_forward = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # Layer normalization layers\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout layers for regularization\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=True, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for encoder layer.\n",
    "\n",
    "        Args:\n",
    "        inputs (tf.Tensor): Input tensor.\n",
    "        training (bool): Whether in training mode.\n",
    "        mask (tf.Tensor, optional): Padding mask.\n",
    "\n",
    "        Returns:\n",
    "        tf.Tensor: Output of the encoder layer.\n",
    "        \"\"\"\n",
    "        # Multi-head self-attention sublayer\n",
    "        # For self-attention, the same tensor is used for query, key, and value\n",
    "        attention_output, _ = self.multi_head_attention(\n",
    "            query=inputs, key=inputs, value=inputs, mask=mask)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        attention_output_normalized = self.layer_norm1(inputs + attention_output)  # Residual connection\n",
    "\n",
    "        # Feed-forward network sublayer\n",
    "        ffn_output = self.feed_forward(attention_output_normalized)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        output_normalized = self.layer_norm2(attention_output_normalized + ffn_output)  # Residual connection\n",
    "\n",
    "        return output_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Encoder Stack\n",
    "\n",
    "The complete encoder consists of N identical layers stacked sequentially. This stacking allows the model to build increasingly abstract and context-aware representations:\n",
    "\n",
    "- The first few layers typically capture more local patterns and syntactic relationships\n",
    "- Deeper layers develop more abstract and semantic representations\n",
    "- The final layer produces the encoder output that will be used by the decoder\n",
    "\n",
    "The number of encoder layers (N) is a hyperparameter that affects the model's capacity and computational requirements. Deeper encoders can capture more complex patterns but require more computation and may be more difficult to train.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder implementation\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of multiple encoder layers.\n",
    "\n",
    "    The encoder processes the input sequence through:\n",
    "    1. Input embedding\n",
    "    2. Positional encoding addition\n",
    "    3. Multiple encoder layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize encoder.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of encoder layers.\n",
    "            d_model (int): Model dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dff (int): Inner dimension of feed-forward networks.\n",
    "            input_vocab_size (int): Size of input vocabulary.\n",
    "            maximum_position_encoding (int): Maximum sequence length for position encoding.\n",
    "            dropout_rate (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Input embedding layer\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        # Stack of encoder layers\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=True, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for encoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input token indices.\n",
    "            training (bool): Whether in training mode.\n",
    "            mask (tf.Tensor, optional): Padding mask.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Encoder output.\n",
    "        \"\"\"\n",
    "        # Get sequence length\n",
    "        sequence_length = tf.shape(inputs)[1]\n",
    "\n",
    "        # Convert token indices to embeddings\n",
    "        embeddings = self.embedding_layer(inputs)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Scale embeddings\n",
    "        embeddings *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        # Add positional encoding\n",
    "        embeddings += self.positional_encoding[:, :sequence_length, :]\n",
    "\n",
    "        # Apply dropout to embeddings\n",
    "        encoder_output = self.dropout(embeddings, training=training)\n",
    "\n",
    "        # Pass through each encoder layer\n",
    "        for layer_index in range(self.num_layers):\n",
    "            encoder_output = self.encoder_layers[layer_index](\n",
    "                encoder_output, training=training, mask=mask)\n",
    "\n",
    "        # Final encoder output shape: (batch_size, seq_len, d_model)\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Components\n",
    "\n",
    "The decoder is the second major component of the Transformer architecture, responsible for generating output sequences based on the encoded representations. Like the encoder, the decoder consists of multiple identical layers stacked on top of each other, but with an additional cross-attention mechanism that connects it to the encoder output.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "Each decoder layer transforms its input through the following sequence of operations:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**: Similar to the encoder's self-attention, but with a crucial difference - it includes a look-ahead mask to prevent positions from attending to future positions\n",
    "   - Input: Sequence representations from the previous decoder layer\n",
    "   - Process: Computes attention scores with masking to ensure autoregressive property\n",
    "   - Output: Context-aware representations that only depend on previous positions\n",
    "\n",
    "2. **Residual Connection and Layer Normalization**: After the masked self-attention sub-layer\n",
    "   - Residual connection: Adds the input to the output of the attention sub-layer\n",
    "   - Layer normalization: Normalizes the features across the embedding dimension\n",
    "\n",
    "3. **Multi-Head Cross-Attention**: Allows each position in the decoder to attend to all positions in the encoder output\n",
    "   - Input: Normalized output from the self-attention sub-layer and encoder output\n",
    "   - Process: Computes attention scores between decoder queries and encoder keys/values\n",
    "   - Output: Representations that incorporate information from the encoder\n",
    "\n",
    "4. **Second Residual Connection and Layer Normalization**: After the cross-attention sub-layer\n",
    "\n",
    "5. **Position-wise Feed-Forward Network**: Identical to the one in the encoder layer\n",
    "   - Input: Normalized output from the cross-attention sub-layer\n",
    "   - Process: Two linear transformations with a ReLU activation in between\n",
    "   - Output: Transformed representations with enhanced feature extraction\n",
    "\n",
    "6. **Third Residual Connection and Layer Normalization**: After the feed-forward sub-layer\n",
    "\n",
    "The masking in the self-attention mechanism is crucial for autoregressive generation, ensuring that predictions for position i can only depend on known outputs at positions less than i.\n",
    "\n",
    "The implementation below creates both the individual decoder layer and the complete decoder stack, following the architecture described in the original Transformer paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.354594Z",
     "iopub.status.busy": "2025-02-27T09:43:42.354319Z",
     "iopub.status.idle": "2025-02-27T09:43:42.367741Z",
     "shell.execute_reply": "2025-02-27T09:43:42.367117Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.354567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decoder layer implementation\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Decoder layer for the Transformer model.\n",
    "\n",
    "    Each decoder layer consists of:\n",
    "    1. Masked multi-head self-attention mechanism\n",
    "    2. Multi-head encoder-decoder attention mechanism\n",
    "    3. Position-wise feed-forward network\n",
    "\n",
    "    All sublayers have residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize decoder layer.\n",
    "\n",
    "        Args:\n",
    "        d_model (int): Model dimension.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        dff (int): Inner dimension of feed-forward network.\n",
    "        dropout_rate (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Masked multi-head self-attention sublayer\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # Multi-head encoder-decoder attention sublayer\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # Feed-forward network sublayer\n",
    "        self.feed_forward = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # Layer normalization layers\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout layers for regularization\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, encoder_output, training=True, look_ahead_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for decoder layer.\n",
    "\n",
    "        Args:\n",
    "        inputs (tf.Tensor): Decoder input tensor.\n",
    "        encoder_output (tf.Tensor): Output from the encoder.\n",
    "        training (bool): Whether in training mode.\n",
    "        look_ahead_mask (tf.Tensor, optional): Mask for masked self-attention.\n",
    "        padding_mask (tf.Tensor, optional): Mask for encoder-decoder attention.\n",
    "\n",
    "        Returns:\n",
    "        tuple: (layer output, self-attention weights, encoder-decoder attention weights)\n",
    "        \"\"\"\n",
    "        # Masked multi-head self-attention sublayer\n",
    "        self_attention_output, self_attention_weights = self.self_attention(\n",
    "            query=inputs, key=inputs, value=inputs, mask=look_ahead_mask)\n",
    "        self_attention_output = self.dropout1(self_attention_output, training=training)\n",
    "        self_attention_normalized = self.layer_norm1(inputs + self_attention_output)  # Residual connection\n",
    "\n",
    "        # Multi-head encoder-decoder attention sublayer\n",
    "        enc_dec_attention_output, enc_dec_attention_weights = self.encoder_decoder_attention(\n",
    "            query=self_attention_normalized, key=encoder_output, value=encoder_output, mask=padding_mask)\n",
    "        enc_dec_attention_output = self.dropout2(enc_dec_attention_output, training=training)\n",
    "        enc_dec_attention_normalized = self.layer_norm2(\n",
    "            self_attention_normalized + enc_dec_attention_output)  # Residual connection\n",
    "\n",
    "        # Feed-forward network sublayer\n",
    "        ffn_output = self.feed_forward(enc_dec_attention_normalized)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        output_normalized = self.layer_norm3(enc_dec_attention_normalized + ffn_output)  # Residual connection\n",
    "\n",
    "        return output_normalized, self_attention_weights, enc_dec_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Decoder Stack\n",
    "\n",
    "The complete decoder consists of N identical layers stacked sequentially, mirroring the structure of the encoder stack. The decoder stack processes its inputs in conjunction with the encoder output:\n",
    "\n",
    "1. The first layer takes positional-encoded embeddings of the target sequence (shifted right)\n",
    "2. Each subsequent layer refines these representations using self-attention, cross-attention with the encoder output, and feed-forward transformations\n",
    "3. The final layer produces representations that are then projected to output probabilities over the vocabulary\n",
    "\n",
    "The decoder's autoregressive property makes it suitable for sequence generation tasks like language modeling, where each token is generated based on all previously generated tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.384173Z",
     "iopub.status.busy": "2025-02-27T09:43:42.383938Z",
     "iopub.status.idle": "2025-02-27T09:43:42.398437Z",
     "shell.execute_reply": "2025-02-27T09:43:42.397880Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.384155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decoder implementation\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of multiple decoder layers.\n",
    "\n",
    "    The decoder processes the target sequence through:\n",
    "    1. Target embedding\n",
    "    2. Positional encoding addition\n",
    "    3. Multiple decoder layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 target_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize decoder.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of decoder layers.\n",
    "            d_model (int): Model dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dff (int): Inner dimension of feed-forward networks.\n",
    "            target_vocab_size (int): Size of target vocabulary.\n",
    "            maximum_position_encoding (int): Maximum sequence length for position encoding.\n",
    "            dropout_rate (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Target embedding layer\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        # Stack of decoder layers\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(d_model, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, encoder_output, training=True, look_ahead_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Target token indices.\n",
    "            encoder_output (tf.Tensor): Output from the encoder.\n",
    "            training (bool): Whether in training mode.\n",
    "            look_ahead_mask (tf.Tensor, optional): Mask for masked self-attention.\n",
    "            padding_mask (tf.Tensor, optional): Mask for encoder-decoder attention.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (decoder output, attention weights dictionary)\n",
    "        \"\"\"\n",
    "        # Get sequence length\n",
    "        sequence_length = tf.shape(inputs)[1]\n",
    "\n",
    "        # Dictionary to store attention weights from each decoder layer\n",
    "        attention_weights = {}\n",
    "\n",
    "        # Convert token indices to embeddings\n",
    "        embeddings = self.embedding_layer(inputs)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Scale embeddings\n",
    "        embeddings *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        # Add positional encoding\n",
    "        embeddings += self.positional_encoding[:, :sequence_length, :]\n",
    "\n",
    "        # Apply dropout to embeddings\n",
    "        decoder_output = self.dropout(embeddings, training=training)\n",
    "\n",
    "        # Pass through each decoder layer\n",
    "        for layer_index in range(self.num_layers):\n",
    "            decoder_output, self_attn_weights, enc_dec_attn_weights = self.decoder_layers[layer_index](\n",
    "                decoder_output,\n",
    "                encoder_output,\n",
    "                training=training,\n",
    "                look_ahead_mask=look_ahead_mask,\n",
    "                padding_mask=padding_mask\n",
    "            )\n",
    "\n",
    "            # Store attention weights for visualization/analysis\n",
    "            attention_weights[f'decoder_layer{layer_index+1}_self_attention'] = self_attn_weights\n",
    "            attention_weights[f'decoder_layer{layer_index+1}_encoder_decoder_attention'] = enc_dec_attn_weights\n",
    "\n",
    "        # Final decoder output shape: (batch_size, seq_len, d_model)\n",
    "        return decoder_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking Functions\n",
    "\n",
    "Masking is a crucial aspect of the Transformer architecture, serving two distinct purposes:\n",
    "\n",
    "1. **Padding Masks**: Handle variable-length sequences in batched processing\n",
    "2. **Look-Ahead Masks**: Ensure the autoregressive property in the decoder\n",
    "\n",
    "These masks modify the attention mechanism to prevent certain connections between positions, effectively controlling information flow within the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Mask Creation\n",
    "\n",
    "When processing batches of sequences, we typically pad shorter sequences to match the length of the longest sequence in the batch. However, we don't want the model to attend to or be influenced by these padding tokens. Padding masks solve this problem:\n",
    "\n",
    "- They identify which positions contain actual tokens versus padding tokens\n",
    "- They're applied to the attention weights before the softmax operation\n",
    "- By setting attention weights for padding tokens to a large negative value (e.g., -1e9), the softmax effectively gives them zero attention\n",
    "\n",
    "Padding masks are used in both the encoder and decoder to ensure that padding tokens don't contribute to the contextual representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.399490Z",
     "iopub.status.busy": "2025-02-27T09:43:42.399191Z",
     "iopub.status.idle": "2025-02-27T09:43:42.413562Z",
     "shell.execute_reply": "2025-02-27T09:43:42.412941Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.399450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for creating masks\n",
    "def create_padding_mask(sequence):\n",
    "    \"\"\"\n",
    "    Create a padding mask for transformer model.\n",
    "\n",
    "    This mask identifies padding tokens (zeros) in the input sequence to ensure\n",
    "    they don't contribute to attention calculations.\n",
    "\n",
    "    Args:\n",
    "        sequence (tf.Tensor): Input sequence tensor of shape (batch_size, seq_len).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Padding mask of shape (batch_size, 1, 1, seq_len).\n",
    "    \"\"\"\n",
    "    # Create mask where padding tokens (0) become 1, and other tokens become 0\n",
    "    mask = tf.cast(tf.math.equal(sequence, 0), tf.float32)\n",
    "\n",
    "    # Add dimensions for multi-head attention broadcasting\n",
    "    # Shape: (batch_size, 1, 1, seq_len)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look-Ahead Mask Creation\n",
    "\n",
    "The look-ahead mask (or causal mask) is specific to the decoder's self-attention mechanism and enforces the autoregressive property:\n",
    "\n",
    "- It prevents each position from attending to future positions\n",
    "- It creates a lower triangular matrix where each position i can only attend to positions j ≤ i\n",
    "- Like padding masks, it works by setting prohibited connections to large negative values\n",
    "\n",
    "This masking is essential for language modeling and text generation tasks, where the model should only condition its predictions on previously generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Create a look-ahead mask for decoder self-attention.\n",
    "\n",
    "    This mask prevents the decoder from attending to future positions during training,\n",
    "    ensuring autoregressive property (can only see previous positions).\n",
    "\n",
    "    Args:\n",
    "        sequence_length (int): Length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Look-ahead mask of shape (sequence_length, sequence_length).\n",
    "    \"\"\"\n",
    "    # Create a lower triangular matrix with ones\n",
    "    # 1s in the lower triangle, 0s in the upper triangle\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((sequence_length, sequence_length)), -1, 0)\n",
    "    return mask  # Shape: (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Masking\n",
    "\n",
    "In practice, we often need to combine both types of masks in the decoder:\n",
    "\n",
    "- The look-ahead mask ensures autoregressive generation\n",
    "- The padding mask ensures we ignore padding tokens\n",
    "- The combined mask applies both constraints simultaneously\n",
    "\n",
    "The helper functions below implement these masking operations, creating the appropriate tensor masks that can be directly applied in the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(encoder_input, decoder_input):\n",
    "    \"\"\"\n",
    "    Create all necessary masks for the transformer model.\n",
    "\n",
    "    Args:\n",
    "        encoder_input (tf.Tensor): Input to the encoder.\n",
    "        decoder_input (tf.Tensor): Input to the decoder.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (encoder padding mask, combined decoder mask, decoder padding mask)\n",
    "    \"\"\"\n",
    "    # Create padding mask for encoder inputs\n",
    "    encoder_padding_mask = create_padding_mask(encoder_input)\n",
    "\n",
    "    # Create padding mask for decoder inputs in encoder-decoder attention\n",
    "    decoder_padding_mask = create_padding_mask(encoder_input)\n",
    "\n",
    "    # Create look-ahead mask for decoder self-attention\n",
    "    decoder_look_ahead_mask = create_look_ahead_mask(tf.shape(decoder_input)[1])\n",
    "\n",
    "    # Create padding mask for decoder inputs in decoder self-attention\n",
    "    decoder_input_padding_mask = create_padding_mask(decoder_input)\n",
    "\n",
    "    # Combine look-ahead mask and padding mask for decoder self-attention\n",
    "    # This prevents attending to future tokens AND padding tokens\n",
    "    combined_decoder_mask = tf.maximum(decoder_look_ahead_mask, decoder_input_padding_mask)\n",
    "\n",
    "    return encoder_padding_mask, combined_decoder_mask, decoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Transformer Model\n",
    "\n",
    "After implementing all the individual components, we can now assemble the complete Transformer architecture. The Transformer model integrates the encoder and decoder stacks, along with embedding layers, positional encoding, and the final output layer to create a powerful sequence-to-sequence model.\n",
    "\n",
    "The complete Transformer architecture consists of:\n",
    "\n",
    "1. **Input and Output Embedding Layers**: Convert token indices to continuous vector representations\n",
    "   - In our implementation, we share the same embedding layer for both input and output tokens\n",
    "   - This parameter sharing is common in language models and reduces the model size\n",
    "\n",
    "2. **Positional Encoding**: Adds information about token positions to the embeddings\n",
    "   - Applied to both encoder and decoder inputs\n",
    "   - Enables the model to understand sequence order despite its parallel processing\n",
    "\n",
    "3. **Encoder Stack**: Processes the input sequence to create contextualized representations\n",
    "   - Consists of N identical layers with self-attention and feed-forward networks\n",
    "   - Captures bidirectional context from the entire input sequence\n",
    "\n",
    "4. **Decoder Stack**: Generates the output sequence based on the encoder output\n",
    "   - Consists of N identical layers with masked self-attention, cross-attention, and feed-forward networks\n",
    "   - Maintains the autoregressive property for sequence generation\n",
    "\n",
    "5. **Final Linear Layer and Softmax**: Projects decoder output to vocabulary-sized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.414620Z",
     "iopub.status.busy": "2025-02-27T09:43:42.414347Z",
     "iopub.status.idle": "2025-02-27T09:43:42.429710Z",
     "shell.execute_reply": "2025-02-27T09:43:42.429003Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.414594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Complete Transformer Model\n",
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete Transformer model as described in 'Attention Is All You Need'.\n",
    "\n",
    "    The Transformer consists of an encoder and a decoder, each with multiple layers\n",
    "    of self-attention and feed-forward networks, connected by encoder-decoder attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size,\n",
    "                 maximum_position_encoding_input, maximum_position_encoding_target,\n",
    "                 dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Transformer model.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of encoder and decoder layers.\n",
    "            d_model (int): Model dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dff (int): Inner dimension of feed-forward networks.\n",
    "            input_vocab_size (int): Size of input vocabulary.\n",
    "            target_vocab_size (int): Size of target vocabulary.\n",
    "            maximum_position_encoding_input (int): Maximum input sequence length for position encoding.\n",
    "            maximum_position_encoding_target (int): Maximum target sequence length for position encoding.\n",
    "            dropout_rate (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Encoder stack\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dff=dff,\n",
    "            input_vocab_size=input_vocab_size,\n",
    "            maximum_position_encoding=maximum_position_encoding_input,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        # Decoder stack\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dff=dff,\n",
    "            target_vocab_size=target_vocab_size,\n",
    "            maximum_position_encoding=maximum_position_encoding_target,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        # Final projection layer to vocabulary\n",
    "        self.final_projection = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            inputs (tuple): (encoder_inputs, decoder_inputs).\n",
    "            training (bool): Whether in training mode.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (output logits, attention weights dictionary)\n",
    "        \"\"\"\n",
    "        # Unpack inputs\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "\n",
    "        # Create masks for encoder and decoder\n",
    "        encoder_padding_mask, combined_decoder_mask, decoder_padding_mask = create_masks(\n",
    "            encoder_inputs, decoder_inputs)\n",
    "\n",
    "        # Pass through encoder\n",
    "        encoder_output = self.encoder(\n",
    "            encoder_inputs,\n",
    "            training=training,\n",
    "            mask=encoder_padding_mask\n",
    "        )\n",
    "\n",
    "        # Pass through decoder\n",
    "        decoder_output, attention_weights = self.decoder(\n",
    "            decoder_inputs,\n",
    "            encoder_output,\n",
    "            training=training,\n",
    "            look_ahead_mask=combined_decoder_mask,\n",
    "            padding_mask=decoder_padding_mask\n",
    "        )\n",
    "\n",
    "        # Project to vocabulary size\n",
    "        final_output = self.final_projection(decoder_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Transformer for Word-Level Language Modeling\n",
    "\n",
    "After implementing the complete Transformer architecture, we need to configure it specifically for our word-level Shakespeare language modeling task. This configuration involves setting appropriate hyperparameters that balance model capacity, computational efficiency, and performance.\n",
    "\n",
    "The function below creates a Transformer model with the following configuration:\n",
    "\n",
    "1. **Model Size and Capacity**:\n",
    "   - **Number of layers**: 3 encoder and decoder layers, providing sufficient depth to capture complex patterns while remaining computationally tractable\n",
    "   - **Embedding dimension (d_model)**: 192-dimensional embeddings, offering a good balance between expressiveness and efficiency\n",
    "   - **Feed-forward dimension (dff)**: 768-dimensional inner layer in the feed-forward networks, allowing for complex non-linear transformations\n",
    "   - **Number of attention heads**: 6 heads, enabling the model to attend to different aspects of the input simultaneously\n",
    "\n",
    "2. **Regularization**:\n",
    "   - **Dropout rate**: 0.3, providing regularization to prevent overfitting on our relatively small dataset\n",
    "\n",
    "3. **Sequence Handling**:\n",
    "   - **Maximum position encoding**: 5000 positions, accommodating long sequences while maintaining positional information\n",
    "   - **Vocabulary size**: Dynamically set based on our preprocessed data, ensuring all tokens in our vocabulary can be represented\n",
    "\n",
    "This configuration is specifically tailored for word-level language modeling, where each token represents a complete word rather than a character. Word-level models typically require:\n",
    "- Larger embedding dimensions to capture the greater semantic complexity of words\n",
    "- Fewer position encodings than character models (since sequences contain fewer tokens)\n",
    "- Careful regularization to handle the larger vocabulary size\n",
    "\n",
    "The resulting model strikes a balance between capacity and efficiency, making it suitable for training on Shakespeare's text without requiring excessive computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.430790Z",
     "iopub.status.busy": "2025-02-27T09:43:42.430495Z",
     "iopub.status.idle": "2025-02-27T09:43:42.906177Z",
     "shell.execute_reply": "2025-02-27T09:43:42.905525Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.430763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create Transformer model for word-level language modeling\n",
    "def create_word_transformer(vocab_size):\n",
    "    \"\"\"\n",
    "    Create a Transformer model configured for word-level language modeling.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        Transformer: Configured model instance.\n",
    "    \"\"\"\n",
    "    # Model architecture hyperparameters\n",
    "    num_layers = 3        # Number of encoder/decoder layers\n",
    "    d_model = 192         # Embedding dimension\n",
    "    num_heads = 6         # Number of attention heads\n",
    "    dff = 768            # Feed-forward network inner dimension\n",
    "    dropout_rate = 0.3    # Dropout rate for regularization\n",
    "\n",
    "    # Vocabulary sizes from the processed data\n",
    "    input_vocab_size = vocab_size\n",
    "    target_vocab_size = vocab_size\n",
    "\n",
    "    # Maximum position encoding length\n",
    "    # For word models, we typically need fewer position encodings than character models\n",
    "    max_position_encoding = 5000\n",
    "\n",
    "    # Create and return the Transformer model\n",
    "    return Transformer(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=dff,\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        maximum_position_encoding_input=max_position_encoding,\n",
    "        maximum_position_encoding_target=max_position_encoding,\n",
    "        dropout_rate=dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer model with default parameters\n",
    "word_transformer = create_word_transformer(vocab_size)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Implementation\n",
    "\n",
    "Training a Transformer model for language modeling requires careful implementation of several components: loss calculation, optimization strategy, learning rate scheduling, and the training loop itself. This section covers the complete training pipeline for our Shakespeare language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function with Label Smoothing\n",
    "\n",
    "For training our Transformer model, we implement a specialized loss function that incorporates label smoothing, a regularization technique that improves model generalization and calibration. Label smoothing addresses several challenges in language model training:\n",
    "\n",
    "1. **Overconfidence**: Neural networks tend to become overly confident in their predictions, assigning probabilities close to 1.0 for the target class. Label smoothing prevents this by \"softening\" the target distribution.\n",
    "\n",
    "2. **Generalization**: By introducing uncertainty into the training targets, label smoothing encourages the model to learn more robust representations rather than memorizing the training data.\n",
    "\n",
    "3. **Calibration**: Models trained with label smoothing typically produce better-calibrated probability distributions, meaning their confidence better reflects their actual accuracy.\n",
    "\n",
    "The implementation works as follows:\n",
    "\n",
    "- **Standard one-hot encoding** would assign a probability of 1.0 to the correct word and 0.0 to all other words\n",
    "- **With label smoothing**, we assign a probability of (1-α) to the correct word and distribute the remaining α probability uniformly across all words in the vocabulary\n",
    "- **Mathematically**: If y is the one-hot encoded ground truth and V is the vocabulary size, the smoothed label becomes:\n",
    "  \n",
    "  $$y_{smooth} = (1-\\alpha) \\cdot y + \\alpha \\cdot \\frac{1}{V}$$\n",
    "\n",
    "Additionally, our loss function handles padding tokens by:\n",
    "- Creating a mask that identifies non-padding tokens (those with ID ≠ 0)\n",
    "- Applying this mask to the loss values to ignore padding tokens\n",
    "- Normalizing the total loss by the number of non-padding tokens\n",
    "\n",
    "This approach ensures that the model is only penalized for its predictions on actual content, not on padding tokens used for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.907212Z",
     "iopub.status.busy": "2025-02-27T09:43:42.906949Z",
     "iopub.status.idle": "2025-02-27T09:43:42.911577Z",
     "shell.execute_reply": "2025-02-27T09:43:42.910672Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.907184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss function for training\n",
    "def loss_function(real_tokens, predicted_logits, smoothing_factor=0.1):\n",
    "    \"\"\"\n",
    "    Calculate the loss with manual label smoothing, ignoring padding tokens.\n",
    "    \n",
    "    Args:\n",
    "        real_tokens (tf.Tensor): Ground truth tokens.\n",
    "        predicted_logits (tf.Tensor): Predicted token logits.\n",
    "        smoothing_factor (float): Label smoothing factor (0.0 to 1.0).\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: The average loss value across the batch.\n",
    "    \"\"\"\n",
    "    # Get vocabulary size from the last dimension of the logits\n",
    "    vocab_size = tf.shape(predicted_logits)[-1]\n",
    "    \n",
    "    # Convert sparse tokens to one-hot\n",
    "    one_hot_labels = tf.one_hot(real_tokens, depth=vocab_size)\n",
    "    \n",
    "    # Apply label smoothing manually:\n",
    "    # - Assign (1-smoothing_factor) probability to the correct class\n",
    "    # - Distribute smoothing_factor probability uniformly to all classes\n",
    "    smooth_labels = (1.0 - smoothing_factor) * one_hot_labels + \\\n",
    "                    smoothing_factor / tf.cast(vocab_size, tf.float32)\n",
    "    \n",
    "    # Calculate cross entropy from logits using smoothed labels\n",
    "    loss_values = tf.keras.losses.categorical_crossentropy(\n",
    "        smooth_labels, predicted_logits, from_logits=True)\n",
    "    \n",
    "    # Create a mask to ignore padding tokens (ID = 0)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real_tokens, 0))\n",
    "    \n",
    "    # Apply mask to ignore padding tokens in loss calculation\n",
    "    mask = tf.cast(mask, dtype=loss_values.dtype)\n",
    "    loss_values *= mask\n",
    "    \n",
    "    # Return average loss (averaging over non-padding tokens only)\n",
    "    return tf.reduce_sum(loss_values) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Learning Rate Scheduler\n",
    "\n",
    "The Transformer architecture benefits significantly from a carefully designed learning rate schedule. The original paper \"Attention Is All You Need\" introduced a specific learning rate schedule that has become standard practice when training Transformer models. This schedule combines two key elements:\n",
    "\n",
    "1. **Warm-up phase**: The learning rate gradually increases during the initial training steps, allowing the model to establish meaningful parameter values before applying larger updates.\n",
    "\n",
    "2. **Decay phase**: After the warm-up, the learning rate decreases proportionally to the inverse square root of the step number, helping the model converge to an optimal solution.\n",
    "\n",
    "The mathematical formula for this schedule is:\n",
    "\n",
    "$$\\text{lr} = \\text{scale} \\cdot d_{\\text{model}}^{-0.5} \\cdot \\min(\\text{step}^{-0.5}, \\text{step} \\cdot \\text{warmup\\_steps}^{-1.5})$$\n",
    "\n",
    "Where:\n",
    "- $d_{\\text{model}}$ is the embedding dimension\n",
    "- $\\text{step}$ is the current training step\n",
    "- $\\text{warmup\\_steps}$ is the number of steps in the warm-up phase\n",
    "- $\\text{scale}$ is an additional scaling factor to fine-tune the overall learning rate magnitude\n",
    "\n",
    "This schedule offers several advantages:\n",
    "\n",
    "- **Stability**: The gradual warm-up prevents unstable gradients early in training\n",
    "- **Efficient exploration**: Higher learning rates during the middle phase allow efficient parameter space exploration\n",
    "- **Fine-tuning**: The gradual decay helps the model settle into an optimal configuration\n",
    "- **Scaling with model size**: The $d_{\\text{model}}^{-0.5}$ factor automatically adjusts the learning rate based on model size\n",
    "\n",
    "The implementation below creates a custom TensorFlow learning rate scheduler that follows this formula, which we'll use with the Adam optimizer for training our Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.915142Z",
     "iopub.status.busy": "2025-02-27T09:43:42.914941Z",
     "iopub.status.idle": "2025-02-27T09:43:42.925334Z",
     "shell.execute_reply": "2025-02-27T09:43:42.924673Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.915125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom learning rate scheduler\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Custom learning rate scheduler for Transformer model.\n",
    "\n",
    "    Implements the learning rate schedule from the Transformer paper:\n",
    "    lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=1000, initial_scale=0.5):\n",
    "        \"\"\"\n",
    "        Initialize learning rate scheduler.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension.\n",
    "            warmup_steps (int): Number of warmup steps. Default is 1000.\n",
    "            initial_scale (float): Additional scaling factor. Default is 0.5.\n",
    "        \"\"\"\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.initial_scale = initial_scale\n",
    "\n",
    "    def __call__(self, step):\n",
    "        \"\"\"\n",
    "        Calculate learning rate based on step count.\n",
    "\n",
    "        Args:\n",
    "            step (tf.Tensor): Current step number.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Learning rate value.\n",
    "        \"\"\"\n",
    "        # Convert step to float32\n",
    "        step = tf.cast(step, tf.float32)\n",
    "\n",
    "        # Calculate args for min function\n",
    "        arg1 = tf.math.rsqrt(step)  # step^(-0.5)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)  # step * warmup_steps^(-1.5)\n",
    "\n",
    "        # Apply formula: scale * d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
    "        return self.initial_scale * tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Configuration\n",
    "\n",
    "Selecting and configuring the right optimizer is crucial for effectively training Transformer models. For our Shakespeare language model, we use the Adam optimizer with carefully tuned hyperparameters and our custom learning rate schedule.\n",
    "\n",
    "Adam (Adaptive Moment Estimation) combines the benefits of two other optimization algorithms:\n",
    "- **AdaGrad**: Adapts learning rates based on the frequency of parameter updates\n",
    "- **RMSProp**: Uses a moving average of squared gradients to normalize updates\n",
    "\n",
    "This makes it particularly well-suited for training deep neural networks with many parameters, such as Transformers. Our optimizer configuration includes several important customizations:\n",
    "\n",
    "1. **Custom learning rate schedule**: Implements the warm-up and decay schedule described in the previous section, which is essential for stable Transformer training\n",
    "\n",
    "2. **Beta parameters**: Controls the exponential decay rates for moment estimates\n",
    "   - **Beta_1 = 0.9**: Standard value for the first moment (mean)\n",
    "   - **Beta_2 = 0.98**: Slightly higher than the default (0.999) for the second moment (variance), as recommended in the original Transformer paper\n",
    "\n",
    "3. **Epsilon**: A small constant (1e-9) added to the denominator for numerical stability, preventing division by zero\n",
    "\n",
    "4. **Weight decay**: A small amount of L2 regularization (0.01) to prevent overfitting by penalizing large weights\n",
    "\n",
    "These carefully selected hyperparameters help balance the trade-offs between:\n",
    "- Training speed and stability\n",
    "- Exploration of parameter space and convergence\n",
    "- Model performance and generalization\n",
    "\n",
    "The function below creates an Adam optimizer with these customizations, which we'll use to train our Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.926601Z",
     "iopub.status.busy": "2025-02-27T09:43:42.926336Z",
     "iopub.status.idle": "2025-02-27T09:43:42.944062Z",
     "shell.execute_reply": "2025-02-27T09:43:42.943389Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.926580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer with learning rate schedule\n",
    "def create_optimizer(d_model=192):\n",
    "    \"\"\"\n",
    "    Create an Adam optimizer with custom learning rate schedule.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): Model dimension for the learning rate schedule.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.optimizers.Adam: Configured optimizer.\n",
    "    \"\"\"\n",
    "    # Create learning rate schedule\n",
    "    learning_rate_schedule = CustomSchedule(\n",
    "        d_model=d_model,  # Match model's embedding dimension\n",
    "        warmup_steps=2000,\n",
    "        initial_scale=0.3\n",
    "    )\n",
    "\n",
    "    # Initialize Adam optimizer with custom learning rate schedule\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate_schedule,\n",
    "        beta_1=0.9,        # Exponential decay rate for 1st moment estimates\n",
    "        beta_2=0.98,       # Exponential decay rate for 2nd moment estimates (slightly higher than default)\n",
    "        epsilon=1e-9,      # Small constant for numerical stability\n",
    "        weight_decay=0.01  # Add L2 regularization    \n",
    "    )\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step Implementation\n",
    "\n",
    "The core of our Transformer training pipeline is the training step function, which executes a single forward and backward pass through the model. This function is optimized using TensorFlow's `@tf.function` decorator, which compiles the computation into a high-performance graph for faster execution.\n",
    "\n",
    "Our training step implements several key techniques for effective sequence-to-sequence training:\n",
    "\n",
    "1. **Teacher Forcing**: During training, we provide the ground truth tokens as input to the decoder, rather than using the decoder's own predictions. This approach:\n",
    "   - Stabilizes training by preventing error accumulation\n",
    "   - Allows parallel training of all output positions\n",
    "   - Creates a consistent learning signal\n",
    "\n",
    "2. **Input-Target Preparation**:\n",
    "   - **Decoder inputs**: The target sequence with the last token removed (since we don't need to predict after the last token)\n",
    "   - **Decoder targets**: The target sequence with the first token removed (since the first token is typically a start token)\n",
    "\n",
    "3. **Gradient Management**:\n",
    "   - **Gradient clipping**: Limits the gradient norm to prevent exploding gradients, a common issue in training deep sequence models\n",
    "   - **Automatic differentiation**: Uses TensorFlow's GradientTape to efficiently compute gradients\n",
    "\n",
    "4. **Metrics Tracking**:\n",
    "   - **Loss**: Tracks the cross-entropy loss with label smoothing\n",
    "   - **Accuracy**: Measures token-level prediction accuracy\n",
    "\n",
    "The training step function encapsulates the entire process of:\n",
    "1. Preparing the input and target sequences\n",
    "2. Performing the forward pass through the model\n",
    "3. Computing the loss with label smoothing\n",
    "4. Calculating and clipping gradients\n",
    "5. Applying the gradients to update model parameters\n",
    "6. Updating training metrics\n",
    "\n",
    "This function will be called repeatedly during the training loop, once for each batch of data in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.944967Z",
     "iopub.status.busy": "2025-02-27T09:43:42.944764Z",
     "iopub.status.idle": "2025-02-27T09:43:42.950211Z",
     "shell.execute_reply": "2025-02-27T09:43:42.949543Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.944949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training step function\n",
    "@tf.function\n",
    "def train_step(model, input_batch, target_batch, optimizer, train_loss_metric, train_accuracy_metric):\n",
    "    \"\"\"\n",
    "    Execute a single training step (forward pass, loss calculation, and backpropagation).\n",
    "\n",
    "    This function implements teacher forcing for sequence-to-sequence training,\n",
    "    where the target sequence is shifted to create decoder inputs and labels.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): The Transformer model.\n",
    "        input_batch (tf.Tensor): Batch of input sequences.\n",
    "        target_batch (tf.Tensor): Batch of target sequences.\n",
    "        optimizer (tf.keras.optimizers.Optimizer): Optimizer instance.\n",
    "        train_loss_metric (tf.keras.metrics.Mean): Metric to track training loss.\n",
    "        train_accuracy_metric (tf.keras.metrics.SparseCategoricalAccuracy): Metric to track training accuracy.\n",
    "    \"\"\"\n",
    "    # Implement teacher forcing:\n",
    "    # - decoder_inputs: target without the last token\n",
    "    # - decoder_targets: target without the first token\n",
    "    decoder_inputs = target_batch[:, :-1]   # Remove the last token (used as input to decoder)\n",
    "    decoder_targets = target_batch[:, 1:]   # Remove the first token (used as ground truth)\n",
    "\n",
    "    # Use gradient tape to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        # Model expects (encoder_inputs, decoder_inputs) as input\n",
    "        predictions, _ = model([input_batch, decoder_inputs], training=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss = loss_function(decoder_targets, predictions, smoothing_factor=0.1)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Apply gradient clipping with norm of 1.0 to prevent explosive gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=1.0)\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Update metrics\n",
    "    train_loss_metric(loss)\n",
    "    train_accuracy_metric(decoder_targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "\n",
    "Evaluating our Transformer model during and after training is essential for monitoring progress, detecting overfitting, and assessing final performance. The evaluation function provides a standardized way to measure the model's performance on validation and test datasets.\n",
    "\n",
    "The evaluation process shares similarities with the training step but with key differences:\n",
    "\n",
    "1. **No Gradient Computation**: During evaluation, we only perform the forward pass through the model without computing or applying gradients, making the process more computationally efficient.\n",
    "\n",
    "2. **No Training-Specific Operations**: Features like dropout are disabled during evaluation to assess the model's true performance.\n",
    "\n",
    "3. **Complete Dataset Processing**: Unlike training, which often reports metrics after each batch, evaluation processes the entire dataset before reporting final metrics.\n",
    "\n",
    "Our evaluation function calculates two primary metrics:\n",
    "\n",
    "1. **Loss**: The same sparse categorical cross-entropy loss used during training, which measures how well the model's probability distributions match the actual next words.\n",
    "\n",
    "2. **Accuracy**: The percentage of tokens that the model predicts correctly, providing an intuitive measure of performance.\n",
    "\n",
    "Like in training, we use teacher forcing during evaluation, providing the ground truth tokens as input to the decoder. This approach allows us to evaluate each prediction position independently, giving a clear picture of the model's capabilities across different sequence positions.\n",
    "\n",
    "The function below implements this evaluation process, processing an entire dataset and returning the average loss and accuracy. These metrics will be used to:\n",
    "- Monitor training progress\n",
    "- Implement early stopping\n",
    "- Compare different model configurations\n",
    "- Assess final model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation function\n",
    "def evaluate_model(model, dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the transformer model on a validation/test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model to evaluate\n",
    "        dataset: TensorFlow dataset containing input and target batches\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (validation_loss, validation_accuracy) as numpy values\n",
    "    \"\"\"\n",
    "    # Initialize metrics to track loss and accuracy\n",
    "    val_loss = tf.keras.metrics.Mean()\n",
    "    val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    # Iterate through all batches in the dataset\n",
    "    for input_batch, target_batch in dataset:\n",
    "        # Prepare decoder inputs (shift targets left) and targets (shift right)\n",
    "        # This creates teacher forcing inputs for the decoder\n",
    "        decoder_inputs = target_batch[:, :-1]  # All but the last token\n",
    "        decoder_targets = target_batch[:, 1:]  # All but the first token\n",
    "        \n",
    "        # Get model predictions (without training)\n",
    "        predictions, _ = model([input_batch, decoder_inputs], training=False)\n",
    "        \n",
    "        # Calculate loss with label smoothing (0.1) to prevent overconfidence\n",
    "        batch_loss = loss_function(decoder_targets, predictions)\n",
    "        \n",
    "        # Update metrics\n",
    "        val_loss.update_state(batch_loss)\n",
    "        val_accuracy.update_state(decoder_targets, predictions)\n",
    "    \n",
    "    # Return final metrics as numpy values\n",
    "    return val_loss.result().numpy(), val_accuracy.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process\n",
    "\n",
    "Training a Transformer model for language modeling is a complex process that requires careful management of data flow, optimization, and model evaluation. This section outlines our complete training pipeline, which combines all the components we've built so far.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Details\n",
    "\n",
    "The training loop orchestrates the entire training process, managing multiple aspects of model training:\n",
    "\n",
    "1. **Epoch-Based Training**: The model is trained for a specified number of epochs, with each epoch processing the entire training dataset once.\n",
    "\n",
    "2. **Batch Processing**: Within each epoch, the data is processed in batches to enable efficient parallel computation and stochastic optimization.\n",
    "\n",
    "3. **Metrics Tracking**: Several metrics are tracked throughout training:\n",
    "   - Training loss and accuracy: Measured on the training data\n",
    "   - Validation loss and accuracy: Measured on the held-out validation data\n",
    "   - Time per epoch: Tracked to monitor training efficiency\n",
    "\n",
    "4. **Model Checkpointing**: The model's weights are saved whenever the validation loss improves, ensuring we retain the best-performing model configuration.\n",
    "\n",
    "5. **Early Stopping**: Training is halted if the validation loss fails to improve for a specified number of consecutive epochs (patience), preventing overfitting and saving computational resources.\n",
    "\n",
    "6. **Progress Reporting**: Regular updates are printed during training to monitor progress, including:\n",
    "   - Batch-level metrics during each epoch\n",
    "   - Epoch summaries with training and validation metrics\n",
    "   - Sample text generation after each epoch to qualitatively assess model capabilities\n",
    "\n",
    "7. **Sample Generation**: After each epoch, the model generates sample text based on a prompt, providing a tangible demonstration of its current capabilities.\n",
    "\n",
    "This comprehensive training process balances efficient optimization with careful monitoring and evaluation, ensuring that we train an effective language model while avoiding common pitfalls like overfitting or unstable training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train_model(model, train_dataset, val_dataset, epochs, optimizer):\n",
    "    \"\"\"\n",
    "    Train the Transformer model for language modeling.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): The Transformer model to train.\n",
    "        train_dataset (tf.data.Dataset): Training dataset.\n",
    "        val_dataset (tf.data.Dataset): Validation dataset.\n",
    "        epochs (int): Number of training epochs.\n",
    "        optimizer (tf.keras.optimizers.Optimizer): Optimizer.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training history with loss and accuracy metrics.\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to track training history\n",
    "    training_history = {\n",
    "        'train_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "\n",
    "    # Set up model checkpointing\n",
    "    checkpoint_directory = \"./checkpoints\"\n",
    "    checkpoint_path = os.path.join(checkpoint_directory, \"transformer.weights.h5\")\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    if not os.path.exists(checkpoint_directory):\n",
    "        os.makedirs(checkpoint_directory)\n",
    "\n",
    "    # Initialize best validation loss for checkpointing\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10  # Number of epochs with no improvement before early stopping\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop for specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Initialize metrics for this epoch\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "        # Training loop\n",
    "        for batch_idx, (input_batch, target_batch) in enumerate(train_dataset):\n",
    "            # Execute a single training step\n",
    "            train_step(model, input_batch, target_batch, optimizer, train_loss, train_accuracy)\n",
    "\n",
    "            # Print progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Batch {batch_idx}: '\n",
    "                      f'Loss = {train_loss.result():.4f}, '\n",
    "                      f'Accuracy = {train_accuracy.result():.4f}')\n",
    "\n",
    "        # Store training metrics for this epoch\n",
    "        train_loss_value = train_loss.result().numpy()\n",
    "        train_accuracy_value = train_accuracy.result().numpy()\n",
    "\n",
    "        # Validation loop\n",
    "        val_loss_value, val_accuracy_value = evaluate_model(model, val_dataset)\n",
    "\n",
    "        # Checkpointing logic\n",
    "        if val_loss_value < best_val_loss:\n",
    "            best_val_loss = val_loss_value\n",
    "            patience_counter = 0\n",
    "            # Save model weights\n",
    "            model.save_weights(checkpoint_path)\n",
    "            print(f\"Model checkpoint saved with validation loss: {val_loss_value:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve. Patience: {patience_counter}/{patience}\")\n",
    "            # Early stopping logic - will break training if validation loss doesn't improve\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch + 1} Summary: '\n",
    "              f'Training Loss = {train_loss_value:.4f}, '\n",
    "              f'Training Accuracy = {train_accuracy_value:.4f}, '\n",
    "              f'Validation Loss = {val_loss_value:.4f}, '\n",
    "              f'Validation Accuracy = {val_accuracy_value:.4f}')\n",
    "\n",
    "        print(f'Time taken for 1 epoch: {time.time() - start_time:.2f} seconds\\n')\n",
    "\n",
    "        # Generate sample text after every epoch\n",
    "        print(\"\\nGenerating sample text:\")\n",
    "        generated_text = generate_text(\n",
    "            model, \"ROMEO:\", word2idx, idx2word,\n",
    "            generation_length=30, temperature=0.7, top_k=10\n",
    "        )\n",
    "        print(generated_text)\n",
    "        print()\n",
    "\n",
    "        # Update training history\n",
    "        training_history['train_loss'].append(train_loss_value)\n",
    "        training_history['train_accuracy'].append(train_accuracy_value)\n",
    "        training_history['val_loss'].append(val_loss_value)\n",
    "        training_history['val_accuracy'].append(val_accuracy_value)\n",
    "\n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Text Generation During Training\n",
    "\n",
    "Generating sample text during the training process provides valuable qualitative feedback on the model's progress. While quantitative metrics like loss and accuracy are important, they don't always reflect the subjective quality of the generated text. By periodically generating text samples, we can:\n",
    "\n",
    "1. **Observe Learning Progress**: Watch how the model's outputs evolve from random words to coherent Shakespeare-like text\n",
    "2. **Detect Specific Issues**: Identify problems like repetition loops, vocabulary limitations, or stylistic inconsistencies\n",
    "3. **Assess Creative Capabilities**: Evaluate the model's ability to generate text that captures Shakespeare's unique style and language\n",
    "\n",
    "Our text generation function implements several important techniques for high-quality language generation:\n",
    "\n",
    "1. **Autoregressive Generation**: Words are generated one at a time, with each new word conditioned on all previously generated words\n",
    "2. **Temperature Sampling**: Controls the randomness of the generation process\n",
    "   - Higher temperature (e.g., 1.0+): More diverse but potentially less coherent text\n",
    "   - Lower temperature (e.g., 0.5): More focused and deterministic but potentially repetitive text\n",
    "\n",
    "3. **Top-k Sampling**: Restricts sampling to only the k most probable next words\n",
    "   - Helps avoid low-probability outputs that might be grammatically incorrect or contextually inappropriate\n",
    "   - Balances between beam search (deterministic) and pure sampling (fully random)\n",
    "\n",
    "4. **Prompt Conditioning**: Allows generation to be steered by providing an initial text prompt\n",
    "   - Can be used to generate dialogue for specific characters\n",
    "   - Helps maintain thematic consistency in the generated text\n",
    "\n",
    "The function below implements this generation process, taking a trained model and a starting prompt, and producing a sequence of text that continues from that prompt in a style consistent with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T09:43:42.951331Z",
     "iopub.status.busy": "2025-02-27T09:43:42.951080Z",
     "iopub.status.idle": "2025-02-27T09:43:42.966690Z",
     "shell.execute_reply": "2025-02-27T09:43:42.965693Z",
     "shell.execute_reply.started": "2025-02-27T09:43:42.951312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Text generation function\n",
    "def generate_text(model, start_string, word2idx, idx2word, generation_length=100, temperature=1.0, top_k=0):\n",
    "    \"\"\"\n",
    "    Generate text using the trained Transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): Trained Transformer model.\n",
    "        start_string (str): Initial text prompt to start generation.\n",
    "        word2idx (dict): Word-to-index mapping.\n",
    "        idx2word (dict): Index-to-word mapping.\n",
    "        generation_length (int): Number of words to generate.\n",
    "        temperature (float): Controls randomness. Higher values increase diversity,\n",
    "                             lower values increase determinism.\n",
    "        top_k (int): If > 0, only sample from the top k most probable words.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text starting with the input prompt.\n",
    "    \"\"\"\n",
    "    # Convert start string to lowercase and split into words\n",
    "    words = start_string.lower().split()\n",
    "\n",
    "    # Convert words to token indices, handling unknown words\n",
    "    input_indices = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in words]\n",
    "\n",
    "    # Create input tensor for the encoder - add batch dimension\n",
    "    encoder_input = tf.expand_dims(input_indices, 0)  # Add batch dimension\n",
    "\n",
    "    # Initialize list to store generated words\n",
    "    generated_words = []\n",
    "\n",
    "    # Generate words one at a time\n",
    "    for _ in range(generation_length):\n",
    "        # Prepare decoder input from what we've generated so far\n",
    "        decoder_input = tf.expand_dims(input_indices, 0)  # Add batch dimension\n",
    "\n",
    "        # Get model predictions\n",
    "        predictions, _ = model([encoder_input, decoder_input], training=False)\n",
    "\n",
    "        # Get the prediction for the next word (last position)\n",
    "        next_word_predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        scaled_predictions = next_word_predictions / temperature\n",
    "\n",
    "        # Apply top-k sampling if specified\n",
    "        if top_k > 0:\n",
    "            # Get top k predictions and their indices\n",
    "            top_k_predictions, top_k_indices = tf.nn.top_k(\n",
    "                scaled_predictions[0, 0], k=top_k)\n",
    "\n",
    "            # Apply softmax to get probabilities for top-k words only\n",
    "            top_k_probabilities = tf.nn.softmax(top_k_predictions)\n",
    "\n",
    "            # Sample from the filtered distribution\n",
    "            top_k_probabilities_reshaped = tf.reshape(top_k_probabilities, (1, -1))\n",
    "            sampled_index = tf.random.categorical(\n",
    "                tf.math.log(top_k_probabilities_reshaped), num_samples=1)[0, 0]\n",
    "\n",
    "            # Get the actual token ID from our top k indices\n",
    "            predicted_id = top_k_indices[sampled_index]\n",
    "        else:\n",
    "            # Sample from the full distribution\n",
    "            scaled_predictions_reshaped = tf.reshape(scaled_predictions, (1, -1))\n",
    "            predicted_id = tf.random.categorical(\n",
    "                scaled_predictions_reshaped, num_samples=1)[0, 0]\n",
    "\n",
    "        # Convert predicted ID to word and add to result\n",
    "        predicted_word = idx2word[predicted_id.numpy()]\n",
    "        generated_words.append(predicted_word)\n",
    "\n",
    "        # Add predicted ID to input for next iteration\n",
    "        input_indices.append(predicted_id.numpy())\n",
    "\n",
    "    # Combine original prompt with generated text\n",
    "    return start_string + ' ' + ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "After implementing our Transformer model for Shakespeare text generation, we now train the model and analyze its performance. This section covers the complete training process, evaluation metrics, and qualitative assessment of the generated text.\n",
    "\n",
    "### Training Process and Metrics\n",
    "\n",
    "The training process involves initializing the model with our chosen architecture, creating the optimizer with our custom learning rate schedule, and training for a specified number of epochs. Throughout training, we track several key metrics:\n",
    "\n",
    "1. **Training and Validation Loss**: Measures how well the model predicts the next word in a sequence. Lower values indicate better performance.\n",
    "\n",
    "2. **Training and Validation Accuracy**: The percentage of tokens that the model predicts correctly. This provides an intuitive measure of model performance.\n",
    "\n",
    "3. **Generation Quality**: By generating sample text after each epoch, we can qualitatively assess how the model's language generation capabilities evolve during training.\n",
    "\n",
    "The training function implements several best practices:\n",
    "- **Model Checkpointing**: Saves the best model based on validation loss\n",
    "- **Early Stopping**: Halts training if validation performance plateaus\n",
    "- **Periodic Evaluation**: Assesses model performance on the validation set after each epoch\n",
    "- **Sample Generation**: Produces text samples to demonstrate current capabilities\n",
    "\n",
    "### Performance Analysis\n",
    "\n",
    "After training, we analyze the model's performance from multiple perspectives:\n",
    "\n",
    "1. **Quantitative Metrics**: Final loss and accuracy values on both training and validation sets\n",
    "2. **Learning Curves**: How loss and accuracy evolve throughout training\n",
    "3. **Generalization Gap**: The difference between training and validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-27T09:44:32.122Z",
     "iopub.execute_input": "2025-02-27T09:43:42.982599Z",
     "iopub.status.busy": "2025-02-27T09:43:42.982329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run the training process\n",
    "def initialize_and_train_model(word2idx, idx2word, vocab_size, train_dataset, val_dataset):\n",
    "    \"\"\"\n",
    "    Initialize the model and start training.\n",
    "    \n",
    "    Args:\n",
    "        word2idx (dict): Word-to-index mapping.\n",
    "        idx2word (dict): Index-to-word mapping.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        train_dataset (tf.data.Dataset): Training dataset.\n",
    "        val_dataset (tf.data.Dataset): Validation dataset.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained model, training history)\n",
    "    \"\"\"\n",
    "    # Create the transformer model\n",
    "    word_transformer = create_word_transformer(vocab_size)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = create_optimizer(d_model=192)\n",
    "    \n",
    "    # Test untrained model's text generation\n",
    "    print(\"Generating text with untrained model:\")\n",
    "    print(generate_text(\n",
    "        word_transformer, \"ROMEO:\", word2idx, idx2word,\n",
    "        generation_length=30, temperature=1.0\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "    # Start model training\n",
    "    epochs = 5  # Number of training epochs\n",
    "    training_history = train_model(\n",
    "        word_transformer, train_dataset, val_dataset, epochs, optimizer\n",
    "    )\n",
    "    \n",
    "    # Load the best model checkpoint after training is complete\n",
    "    print(\"\\nLoading the best model checkpoint based on validation loss...\")\n",
    "    checkpoint_path = os.path.join(\"./checkpoints\", \"transformer.weights.h5\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        word_transformer.load_weights(checkpoint_path)\n",
    "        print(f\"Best model loaded with validation loss: {min(training_history['val_loss']):.4f}\")\n",
    "    \n",
    "    # Display final metrics after loading best model\n",
    "    print(\"\\nEvaluating best model on validation data...\")\n",
    "    val_loss, val_accuracy = evaluate_model(word_transformer, val_dataset)\n",
    "    print(f\"Best model validation metrics - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Display final training metrics\n",
    "    print(\"\\nTraining complete! Final metrics:\")\n",
    "    print(f\"Training Loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Training Accuracy: {training_history['train_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Validation Loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Validation Accuracy: {training_history['val_accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    return word_transformer, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "word_transformer, training_history = initialize_and_train_model(word2idx, idx2word, vocab_size, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results and Analysis\n",
    "\n",
    "### Initial State and Rapid Improvement\n",
    "\n",
    "Our Transformer model for Shakespearean text generation began with completely random outputs, as evidenced by the initial generated text:\n",
    "\n",
    "\"ROMEO: forewarn toge lodging conveying achieve mummers proudest bianco's true-disposing rein rest sight-outrunning traitors' strings scripture highest unmanner'd wine works helmed guerdon'd answers strew kam rebuke traitorly schoolboys' whoever through't re-quicken'd\"\n",
    "\n",
    "This random collection of Shakespearean vocabulary demonstrates the untrained model's lack of understanding of language structure, grammar, or context.\n",
    "\n",
    "The first epoch showed dramatic improvement, with the loss decreasing from 9.5020 to 3.5426 and accuracy increasing from 0% to 63.37%. This rapid initial learning is typical in language models, as they quickly learn basic patterns like common word sequences and simple grammatical structures. The validation metrics (loss: 2.3887, accuracy: 86.71%) were significantly better than the training metrics, suggesting that our model was generalizing well rather than overfitting.\n",
    "\n",
    "### Learning Progression Across Epochs\n",
    "\n",
    "| Epoch | Training Loss | Training Accuracy | Validation Loss | Validation Accuracy | Time (s) |\n",
    "|-------|---------------|-------------------|-----------------|---------------------|----------|\n",
    "| 1     | 3.5426        | 63.37%            | 2.3887          | 86.71%              | 747.30   |\n",
    "| 2     | 1.8419        | 90.68%            | 2.3412          | 86.84%              | 739.49   |\n",
    "| 3     | 1.7879        | 91.58%            | 2.3277          | 86.83%              | 734.49   |\n",
    "| 4     | 1.7643        | 91.97%            | 2.3468          | 86.34%              | 737.69   |\n",
    "| 5     | 1.7497        | 92.22%            | 2.3575          | 86.10%              | 735.58   |\n",
    "\n",
    "The learning curve shows a classic pattern:\n",
    "- **Epoch 1-2**: Steep improvement in both training and validation metrics\n",
    "- **Epoch 3**: Peak validation performance (lowest validation loss of 2.3277)\n",
    "- **Epochs 4-5**: Continued improvement in training metrics but declining validation performance, indicating the onset of overfitting\n",
    "\n",
    "### Text Generation Quality\n",
    "\n",
    "The quality of generated text improved substantially across epochs:\n",
    "\n",
    "**Epoch 1**:\n",
    "\n",
    "\"ROMEO: , would more of death , that word in time . let death , that in me no more : one , no more , an one , if more\"\n",
    "\n",
    "**Epoch 3** (Best model):\n",
    "\n",
    "\"ROMEO: ? i cannot , do what i cannot , that i say , i can do what , let me , and to me , if i would say ,\"\n",
    "\n",
    "**Epoch 5**:\n",
    "\n",
    "\"ROMEO: , but in the law , if she is a woman , if it is , but the better that which in it once , but this purpose , and\"\n",
    "\n",
    "The progression shows:\n",
    "1. **Basic grammatical structure** emerged quickly\n",
    "2. **Coherent phrases** developed by the middle epochs\n",
    "3. **Contextual relevance** improved, with the model generating text that resembles Shakespearean dialogue\n",
    "\n",
    "However, the generated text still lacks long-term coherence and complex narrative structure, which would require a larger model and more training data.\n",
    "\n",
    "### Overfitting Analysis\n",
    "\n",
    "The divergence between training and validation metrics after epoch 3 indicates the beginning of overfitting. While the training accuracy continued to improve (reaching 92.22% by epoch 5), the validation loss increased from 2.3277 in epoch 3 to 2.3575 in epoch 5. This pattern suggests that the model was starting to memorize specific patterns in the training data rather than learning generalizable features.\n",
    "\n",
    "The early stopping mechanism correctly identified epoch 3 as the optimal stopping point, saving the best model with a validation loss of 2.3277 and accuracy of 86.83%.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our Transformer model successfully learned to generate Shakespearean-style text with reasonable grammatical structure and vocabulary usage. The model achieved its best performance at epoch 3, with a good balance between fitting the training data and generalizing to unseen examples. The generated text shows clear stylistic elements of Shakespeare's writing, though it lacks the complex narrative structure and deep thematic elements of the original works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Metrics Visualization\n",
    "\n",
    "Visualizing training metrics provides valuable insights into the model's learning dynamics and helps identify patterns that might not be apparent from numerical data alone. The plots below illustrate the progression of loss and accuracy metrics for both training and validation sets across the training epochs.\n",
    "\n",
    "### Key Observations from the Visualization:\n",
    "\n",
    "1. **Loss Convergence Pattern**:\n",
    "   - The training loss (blue line) shows a dramatic decrease during the first epoch, dropping from approximately 3.6 to 1.9\n",
    "   - After the first epoch, the training loss continues to decrease but at a much slower rate, indicating diminishing returns from additional training\n",
    "   - The validation loss (red line) remains relatively stable throughout training, with a slight decrease in the early epochs followed by a gradual increase\n",
    "\n",
    "2. **Accuracy Progression**:\n",
    "   - Training accuracy (blue line) shows rapid improvement in the first epoch, jumping from around 65% to over 90%\n",
    "   - The accuracy continues to improve gradually in subsequent epochs, reaching approximately 92% by the end\n",
    "   - Validation accuracy (red line) peaks early and then shows a slight downward trend, suggesting the onset of overfitting\n",
    "\n",
    "3. **Training-Validation Gap**:\n",
    "   - A significant gap develops between training and validation metrics after the first epoch\n",
    "   - This gap widens as training progresses, which is a classic indicator of overfitting\n",
    "   - The model continues to improve on the training data while its performance on unseen data (validation set) begins to deteriorate\n",
    "\n",
    "4. **Optimal Stopping Point**:\n",
    "   - The validation loss reaches its minimum around epoch 3, suggesting this would be the optimal point to stop training\n",
    "   - Continuing beyond this point yields diminishing returns and potentially harmful overfitting\n",
    "\n",
    "These visualizations confirm our earlier observations about the model's learning dynamics and validate the effectiveness of our early stopping strategy, which saved the model from epoch 3 as the best-performing version based on validation loss.\n",
    "\n",
    "The function below creates these visualization plots, displaying both loss and accuracy metrics side by side for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-27T09:44:32.123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualization of training progress\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics over epochs.\n",
    "\n",
    "    Args:\n",
    "        history (dict): Training history dictionary with metrics.\n",
    "    \"\"\"\n",
    "    # Create a figure with two subplots\n",
    "    fig, (loss_axis, accuracy_axis) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot loss metrics\n",
    "    loss_axis.plot(history['train_loss'], 'b-', label='Training Loss')\n",
    "    loss_axis.plot(history['val_loss'], 'r-', label='Validation Loss')\n",
    "    loss_axis.set_xlabel('Epoch')\n",
    "    loss_axis.set_ylabel('Loss')\n",
    "    loss_axis.set_title('Training and Validation Loss')\n",
    "    loss_axis.legend()\n",
    "    loss_axis.grid(True)\n",
    "\n",
    "    # Plot accuracy metrics\n",
    "    accuracy_axis.plot(history['train_accuracy'], 'b-', label='Training Accuracy')\n",
    "    accuracy_axis.plot(history['val_accuracy'], 'r-', label='Validation Accuracy')\n",
    "    accuracy_axis.set_xlabel('Epoch')\n",
    "    accuracy_axis.set_ylabel('Accuracy')\n",
    "    accuracy_axis.set_title('Training and Validation Accuracy')\n",
    "    accuracy_axis.legend()\n",
    "    accuracy_axis.grid(True)\n",
    "\n",
    "    # Adjust layout and display plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of training history\n",
    "plot_training_history(history=training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence\n",
    "\n",
    "Saving and loading trained models is a critical aspect of machine learning workflows, allowing us to:\n",
    "1. Preserve the results of computationally expensive training processes\n",
    "2. Deploy models in production environments\n",
    "3. Share models with other researchers or users\n",
    "4. Resume work with trained models without retraining\n",
    "\n",
    "For our Shakespeare text generation Transformer, we implement a comprehensive model persistence system that saves both the model weights and its configuration parameters.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model and Configuration\n",
    "\n",
    "Properly saving a Transformer model requires preserving two key components:\n",
    "\n",
    "1. **Model Weights**: The learned parameters that encode the knowledge acquired during training. These weights represent the model's ability to generate Shakespeare-like text.\n",
    "\n",
    "2. **Model Architecture Configuration**: The structural parameters that define the model's architecture, such as:\n",
    "   - Number of encoder/decoder layers\n",
    "   - Embedding dimension (d_model)\n",
    "   - Number of attention heads\n",
    "   - Feed-forward network dimensions\n",
    "   - Vocabulary sizes\n",
    "   - Maximum position encoding lengths\n",
    "   - Dropout rates\n",
    "\n",
    "Our saving function handles both aspects:\n",
    "- Weights are saved in HDF5 format, a standard for efficient storage of large numerical arrays\n",
    "- Configuration is saved as a JSON file, providing human-readable documentation of the model's architecture\n",
    "\n",
    "This approach ensures that we can fully reconstruct the model without needing to remember or document the specific hyperparameters used during its creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-27T09:44:32.123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save model function\n",
    "def save_model(model, filepath='./saved_model/shakespeare_transformer'):\n",
    "    \"\"\"\n",
    "    Save the trained Transformer model and its configuration.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): Trained model to save.\n",
    "        filepath (str): Directory path to save model.\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "    # Save model weights\n",
    "    weights_path = os.path.join(filepath, 'model.weights.h5')\n",
    "    model.save_weights(weights_path)\n",
    "    print(f\"Model weights saved to {weights_path}\")\n",
    "\n",
    "    # Extract and save model configuration\n",
    "    model_config = {\n",
    "        'num_layers': model.encoder.num_layers,\n",
    "        'd_model': model.encoder.d_model,\n",
    "        'num_heads': model.encoder.encoder_layers[0].multi_head_attention.num_heads,\n",
    "        'dff': model.encoder.encoder_layers[0].feed_forward.layers[0].units,\n",
    "        'input_vocab_size': model.encoder.embedding_layer.input_dim,\n",
    "        'target_vocab_size': model.decoder.embedding_layer.input_dim,\n",
    "        'maximum_position_encoding_input': model.encoder.positional_encoding.shape[1],\n",
    "        'maximum_position_encoding_target': model.decoder.positional_encoding.shape[1],\n",
    "        'dropout_rate': model.encoder.dropout.rate\n",
    "    }\n",
    "\n",
    "    # Save configuration to JSON file\n",
    "    config_path = os.path.join(filepath, 'model_config.json')\n",
    "    with open(config_path, 'w') as config_file:\n",
    "        json.dump(model_config, config_file, indent=2)\n",
    "    print(f\"Model configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model from Saved Files\n",
    "\n",
    "The complementary loading function reverses this process:\n",
    "1. Reads the configuration file to determine the model's architecture\n",
    "2. Instantiates a new Transformer model with the same architecture\n",
    "3. Loads the saved weights into this model\n",
    "\n",
    "This creates an exact replica of our trained model, ready for text generation or further training. The loading process is designed to be robust, with clear error messages if files are missing or corrupted.\n",
    "\n",
    "The functions below implement this persistence system, providing a reliable way to save and restore our Shakespeare Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model function\n",
    "def load_model(filepath='./saved_model/shakespeare_transformer'):\n",
    "    \"\"\"\n",
    "    Load a Transformer model from saved weights and configuration.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Directory path containing saved model.\n",
    "\n",
    "    Returns:\n",
    "        Transformer: Loaded model instance.\n",
    "    \"\"\"\n",
    "    # Load model configuration\n",
    "    config_path = os.path.join(filepath, 'model_config.json')\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    # Create model instance with loaded configuration\n",
    "    model = Transformer(\n",
    "        num_layers=config['num_layers'],\n",
    "        d_model=config['d_model'],\n",
    "        num_heads=config['num_heads'],\n",
    "        dff=config['dff'],\n",
    "        input_vocab_size=config['input_vocab_size'],\n",
    "        target_vocab_size=config['target_vocab_size'],\n",
    "        maximum_position_encoding_input=config['maximum_position_encoding_input'],\n",
    "        maximum_position_encoding_target=config['maximum_position_encoding_target'],\n",
    "        dropout_rate=config['dropout_rate']\n",
    "    )\n",
    "\n",
    "    # Build the model by calling it once with dummy data\n",
    "    dummy_encoder_input = tf.zeros((1, seq_length), dtype=tf.int32)  # (batch_size, seq_length)\n",
    "    dummy_decoder_input = tf.zeros((1, seq_length), dtype=tf.int32)\n",
    "    _ = model([dummy_encoder_input, dummy_decoder_input], training=False)\n",
    "\n",
    "    # Load model weights\n",
    "    weights_path = os.path.join(filepath, 'model.weights.h5')\n",
    "    model.load_weights(weights_path)\n",
    "    print(f\"Model loaded successfully from {filepath}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(word_transformer, './saved_model/shakespeare_transformer')\n",
    "\n",
    "# Load the model\n",
    "word_transformer = load_model(filepath='./saved_model/shakespeare_transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After training our Transformer model for Shakespeare text generation, we need to evaluate its performance comprehensively. This evaluation goes beyond the training and validation metrics to assess how well the model actually performs its intended task: generating Shakespeare-like text. We'll examine the model from multiple perspectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity Calculation\n",
    "\n",
    "Perplexity is the standard quantitative metric for evaluating language models. It measures how \"surprised\" the model is by the test data, with lower values indicating better performance. Mathematically, perplexity is defined as the exponentiated average negative log-likelihood of a sequence:\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log p(x_i|x_1, \\ldots, x_{i-1})\\right)$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of tokens in the sequence\n",
    "- $p(x_i|x_1, \\ldots, x_{i-1})$ is the probability the model assigns to the actual token $x_i$ given the preceding tokens\n",
    "\n",
    "Intuitively, perplexity can be interpreted as the weighted average number of choices the model is uncertain about when predicting the next token. A perfect model would have a perplexity of 1.0, while random guessing would result in a perplexity equal to the vocabulary size.\n",
    "\n",
    "For our Shakespeare model, we calculate perplexity on the held-out test set to get an unbiased assessment of its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-27T09:44:32.123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate model perplexity\n",
    "def calculate_perplexity(model, dataset):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of the model on a dataset.\n",
    "\n",
    "    Perplexity is a measure of how well a language model predicts a text sample.\n",
    "    Lower perplexity values indicate better performance.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): Trained model.\n",
    "        dataset (tf.data.Dataset): Evaluation dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: Perplexity value.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Evaluate on dataset\n",
    "    for input_batch, target_batch in dataset:\n",
    "        # Teacher forcing for evaluation\n",
    "        decoder_inputs = target_batch[:, :-1]\n",
    "        decoder_targets = target_batch[:, 1:]\n",
    "\n",
    "        # Get model predictions\n",
    "        predictions, _ = model([input_batch, decoder_inputs], training=False)\n",
    "\n",
    "        # Calculate token-level loss\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "        token_loss = loss_object(decoder_targets, predictions)\n",
    "\n",
    "        # Create mask to ignore padding tokens\n",
    "        mask = tf.cast(\n",
    "            tf.math.logical_not(tf.math.equal(decoder_targets, 0)),\n",
    "            dtype=token_loss.dtype\n",
    "        )\n",
    "\n",
    "        # Apply mask\n",
    "        masked_loss = token_loss * mask\n",
    "\n",
    "        # Sum loss and count tokens\n",
    "        total_loss += tf.reduce_sum(masked_loss).numpy()\n",
    "        total_tokens += tf.reduce_sum(mask).numpy()\n",
    "\n",
    "    # Calculate average per-token loss\n",
    "    avg_loss = total_loss / total_tokens\n",
    "\n",
    "    # Perplexity is exp(average_loss)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using perplexity metric on test data\n",
    "print(f\"Test set perplexity: {calculate_perplexity(word_transformer, test_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Text Quality Assessment\n",
    "\n",
    "While perplexity provides a quantitative measure, the ultimate test of a language model is the quality of the text it generates. We evaluate this qualitative aspect by generating text samples with various prompts and generation settings:\n",
    "\n",
    "1. **Different prompts**: We use iconic lines from Shakespeare's plays as starting points to assess how well the model continues in the appropriate style and context.\n",
    "\n",
    "2. **Temperature variation**: The temperature parameter controls randomness in generation:\n",
    "   - Low temperature (e.g., 0.7): More conservative, focused outputs\n",
    "   - Medium temperature (e.g., 1.0): Balanced creativity and coherence\n",
    "   - High temperature (e.g., 1.3): More creative but potentially less coherent outputs\n",
    "\n",
    "3. **Top-k sampling**: This parameter limits token selection to the k most probable next words:\n",
    "   - k=0: Use the full vocabulary distribution\n",
    "   - k=10: Focus on the 10 most likely next words\n",
    "   - k=40: Wider but still constrained selection\n",
    "\n",
    "By systematically varying these parameters, we can explore the model's creative capabilities and find the optimal settings for different generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-27T09:44:32.123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate creative text samples with different settings\n",
    "def generate_creative_samples(model, prompts, word2idx, idx2word):\n",
    "    \"\"\"\n",
    "    Generate text samples with various settings for creative exploration.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): Trained model.\n",
    "        prompts (list): List of text prompts to start generation.\n",
    "        word2idx (dict): Word-to-index mapping.\n",
    "        idx2word (dict): Index-to-word mapping.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing generation results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Define different temperature and top-k settings to try\n",
    "    temperatures = [0.7, 1.0, 1.3]  # Controls randomness: low=conservative, high=creative\n",
    "    top_k_values = [0, 10, 40]      # Controls diversity: 0=all vocabulary, higher=more focused\n",
    "\n",
    "    print(\"Generating creative text samples with different settings:\")\n",
    "\n",
    "    # Generate text for each prompt and setting combination\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "\n",
    "        for temperature in temperatures:\n",
    "            for top_k in top_k_values:\n",
    "                # Generate text with current settings\n",
    "                sample = generate_text(\n",
    "                    model,\n",
    "                    prompt,\n",
    "                    word2idx,\n",
    "                    idx2word,\n",
    "                    generation_length=40,\n",
    "                    temperature=temperature,\n",
    "                    top_k=top_k\n",
    "                )\n",
    "\n",
    "                # Display generated text\n",
    "                print(f\"\\nTemperature={temperature}, Top-K={top_k}:\")\n",
    "                print(sample)\n",
    "\n",
    "                # Store result\n",
    "                results.append({\n",
    "                    'prompt': prompt,\n",
    "                    'temperature': temperature,\n",
    "                    'top_k': top_k,\n",
    "                    'text': sample\n",
    "                })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts\n",
    "prompts = [\n",
    "     \"HAMLET: To be, or not to be,\",\n",
    "     \"ROMEO: But, soft! what light through yonder\",\n",
    "     \"MACBETH: Is this a dagger which I see before me,\",\n",
    "     \"KING LEAR: How sharper than a serpent's tooth it is\"\n",
    " ]\n",
    " \n",
    "creative_samples = generate_creative_samples(word_transformer, prompts, word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benchmarking\n",
    "\n",
    "Beyond quality, we also evaluate the model's practical performance characteristics:\n",
    "\n",
    "#### Inference Speed\n",
    "\n",
    "Generation speed is crucial for applications requiring real-time or near-real-time responses. We measure:\n",
    "- Time required to generate a fixed number of words\n",
    "- Words generated per second\n",
    "- Consistency of generation speed across multiple runs\n",
    "\n",
    "These metrics help assess whether the model is suitable for interactive applications or batch processing scenarios.\n",
    "\n",
    "#### Model Size Analysis\n",
    "\n",
    "The model's resource requirements affect where and how it can be deployed:\n",
    "- Number of trainable and non-trainable parameters\n",
    "- Memory footprint (in MB)\n",
    "- Computational complexity\n",
    "\n",
    "This analysis helps determine whether the model can be deployed on resource-constrained environments or if it requires high-performance computing resources.\n",
    "\n",
    "Together, these evaluation approaches provide a comprehensive assessment of our Shakespeare Transformer model's capabilities, limitations, and practical utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-27T09:44:32.123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Performance benchmarking\n",
    "def benchmark_inference(model, prompt, word2idx, idx2word, num_words=100, num_runs=5):\n",
    "    \"\"\"\n",
    "    Benchmark the inference speed and model size.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): Trained model.\n",
    "        prompt (str): Starting text for generation.\n",
    "        word2idx (dict): Word-to-index mapping.\n",
    "        idx2word (dict): Index-to-word mapping.\n",
    "        num_words (int): Number of words to generate per run.\n",
    "        num_runs (int): Number of runs for averaging results.\n",
    "\n",
    "    Returns:\n",
    "        dict: Benchmark results.\n",
    "    \"\"\"\n",
    "    generation_times = []\n",
    "\n",
    "    # Run multiple generations and measure time\n",
    "    for run_idx in range(num_runs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generate text\n",
    "        _ = generate_text(\n",
    "            model, prompt, word2idx, idx2word,\n",
    "            generation_length=num_words\n",
    "        )\n",
    "\n",
    "        # Record time\n",
    "        end_time = time.time()\n",
    "        generation_times.append(end_time - start_time)\n",
    "\n",
    "    # Calculate average generation time\n",
    "    avg_generation_time = sum(generation_times) / len(generation_times)\n",
    "    words_per_second = num_words / avg_generation_time\n",
    "\n",
    "    # Display time metrics\n",
    "    print(f\"Average generation time: {avg_generation_time:.4f} seconds for {num_words} words\")\n",
    "    print(f\"Generation speed: {words_per_second:.2f} words per second\")\n",
    "\n",
    "    # Calculate model size\n",
    "    trainable_params = sum(\n",
    "        np.prod(variable.shape) for variable in model.trainable_variables\n",
    "    )\n",
    "    non_trainable_params = sum(\n",
    "        np.prod(variable.shape) for variable in model.non_trainable_variables\n",
    "    )\n",
    "    total_params = trainable_params + non_trainable_params\n",
    "\n",
    "    # Display model size information\n",
    "    print(f\"Model parameters: {trainable_params:,} trainable, {non_trainable_params:,} non-trainable\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Approximate model size: {total_params * 4 / (1024 * 1024):.2f} MB (assuming float32)\")\n",
    "\n",
    "    # Return comprehensive benchmark results\n",
    "    return {\n",
    "        'avg_generation_time': avg_generation_time,\n",
    "        'words_per_second': words_per_second,\n",
    "        'trainable_params': int(trainable_params),\n",
    "        'non_trainable_params': int(non_trainable_params),\n",
    "        'total_params': int(total_params),\n",
    "        'individual_times': generation_times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model by generating text starting with \"HAMLET: \"\n",
    "benchmark_inference(word_transformer, \"HAMLET: \", word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Future Work\n",
    "\n",
    "This project has successfully implemented a complete Transformer architecture from scratch for Shakespearean text generation. By training on Shakespeare's works, we've created a model capable of generating text that captures aspects of the Bard's distinctive style and vocabulary. The implementation demonstrates the power of attention-based models for creative language generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Observations and Limitations\n",
    "\n",
    "Throughout this project, several key observations emerged:\n",
    "\n",
    "1. **Learning Dynamics**: The model showed rapid initial learning, with most improvements occurring in the first epoch. This suggests that basic patterns in Shakespeare's language are relatively easy to learn, while the nuanced stylistic elements require more training.\n",
    "\n",
    "2. **Overfitting Patterns**: Despite our regularization efforts, the model began to overfit after the third epoch, with validation loss increasing while training metrics continued to improve. This highlights the challenge of generalization with limited training data.\n",
    "\n",
    "3. **Text Generation Quality**: The generated text successfully captured Shakespearean vocabulary and basic sentence structures. However, it struggled with:\n",
    "   - Long-term coherence beyond a few sentences\n",
    "   - Complex narrative structures\n",
    "   - Consistent character development\n",
    "   - Thematic depth characteristic of Shakespeare's works\n",
    "\n",
    "4. **Hyperparameter Sensitivity**: Text quality was highly sensitive to generation parameters like temperature and top-k sampling. Lower temperatures produced more coherent but less creative text, while higher temperatures increased diversity at the cost of occasional grammatical errors.\n",
    "\n",
    "5. **Model Size Constraints**: With approximately 10 million parameters, our model is relatively small compared to state-of-the-art language models with billions of parameters. This limited capacity constrains the model's ability to capture the full complexity of Shakespeare's writing.\n",
    "\n",
    "6. **Training Efficiency**: The custom learning rate schedule with warmup proved effective, allowing stable training without excessive hyperparameter tuning. However, the training process remained computationally intensive, requiring significant time even on modern hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements and Experimentation Ideas\n",
    "\n",
    "Several promising directions could enhance this project:\n",
    "\n",
    "1. **Architecture Enhancements**:\n",
    "   - Implement relative positional encodings instead of absolute positions\n",
    "   - Explore Transformer-XL or similar architectures for better handling of long-range dependencies\n",
    "   - Experiment with different attention mechanisms like sparse attention\n",
    "\n",
    "2. **Training Improvements**:\n",
    "   - Pre-train on a larger corpus of Elizabethan English before fine-tuning on Shakespeare\n",
    "   - Implement curriculum learning, starting with simpler texts and gradually introducing more complex works\n",
    "   - Explore more sophisticated regularization techniques like stochastic depth or LayerDrop\n",
    "\n",
    "3. **Generation Strategies**:\n",
    "   - Implement nucleus sampling (top-p) as an alternative to top-k\n",
    "   - Add beam search for more coherent generation\n",
    "   - Develop controlled generation techniques to maintain consistent characters or themes\n",
    "\n",
    "4. **Evaluation Enhancements**:\n",
    "   - Develop automated metrics specific to Shakespearean style\n",
    "   - Conduct human evaluation studies comparing generated text to actual Shakespeare\n",
    "   - Analyze the model's ability to capture specific Shakespearean devices like iambic pentameter\n",
    "\n",
    "5. **Application Extensions**:\n",
    "   - Create a character-aware model that can generate text in the style of specific Shakespeare characters\n",
    "   - Develop a dialogue generation system for creating new scenes between Shakespeare characters\n",
    "   - Build an interactive system allowing users to collaborate with the model in writing Shakespeare-inspired content\n",
    "\n",
    "6. **Efficiency Optimizations**:\n",
    "   - Implement model quantization to reduce memory footprint\n",
    "   - Explore knowledge distillation to create smaller, faster models\n",
    "   - Optimize the inference process for real-time generation\n",
    "\n",
    "By pursuing these improvements, future iterations could create even more convincing Shakespeare-style text generation while addressing the limitations observed in the current implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). *Attention Is All You Need*. Advances in Neural Information Processing Systems, 30. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "2. Karpathy, A. (2015). *The Unreasonable Effectiveness of Recurrent Neural Networks*. [Blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "3. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "4. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). *Language Models are Unsupervised Multitask Learners*. OpenAI Blog.\n",
    "\n",
    "5. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). *Transformers: State-of-the-Art Natural Language Processing*. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 38-45).\n",
    "\n",
    "6. Holtzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2020). *The Curious Case of Neural Text Degeneration*. International Conference on Learning Representations. [arXiv:1904.09751](https://arxiv.org/abs/1904.09751)\n",
    "\n",
    "7. Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019). *fairseq: A Fast, Extensible Toolkit for Sequence Modeling*. In Proceedings of NAACL-HLT 2019: Demonstrations.\n",
    "\n",
    "8. Sennrich, R., Haddow, B., & Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units*. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1715-1725).\n",
    "\n",
    "9. Kingma, D. P., & Ba, J. (2014). *Adam: A Method for Stochastic Optimization*. [arXiv:1412.6980](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "10. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., ... & Zheng, X. (2016). *TensorFlow: A System for Large-Scale Machine Learning*. In 12th USENIX Symposium on Operating Systems Design and Implementation (pp. 265-283).\n",
    "\n",
    "11. Shakespeare, W. *The Complete Works of William Shakespeare*. [Project Gutenberg](https://www.gutenberg.org/ebooks/100)\n",
    "\n",
    "12. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & Chintala, S. (2019). *PyTorch: An Imperative Style, High-Performance Deep Learning Library*. Advances in Neural Information Processing Systems, 32, 8026-8037."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
